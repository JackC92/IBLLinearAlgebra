\documentclass[14pt]{problemset}
\usepackage{amsmath}

\usepackage{lipsum}
%\usepackage{showframe}
\usepackage{layout}


\usepackage[charter,cal=cmcal]{mathdesign} %different font
\usepackage{microtype}
\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{xparse}
\usepackage{ifthen}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{calc}
\usepackage{wrapfig}
\usepackage[hidelinks]{hyperref}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
%%%
% Useful Linear Algebra macros
%%%
\newcommand{\ul}{$\underline{\phantom{xxx}}$}
\newcommand{\ull}{\underline{\phantom{xxx}}}
\newcommand{\xh}{{{\mathbf e}_1}}
\newcommand{\yh}{{{\mathbf e}_2}}
\newcommand{\zh}{{{\mathbf e}_3}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\Proj}{\mathrm{proj}}
\newcommand{\Perp}{\mathrm{perp}}
\renewcommand{\span}{\mathrm{span}\,}
\newcommand{\Span}{\mathrm{span}\,}
%\newcommand{\Img}{\mathrm{img}\,}
\newcommand{\Null}{\mathrm{null}\,}
%\newcommand{\Range}{\mathrm{range}\,}
\newcommand{\rref}{\mathrm{rref}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Rank}{\operatorname{rank}}
\newcommand{\nnul}{\operatorname{nullity}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\chr}{\mathrm{char}}
\newcommand{\Char}{\mathrm{char}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\Kern}{\operatorname{kern}}
\newcommand{\Range}{\operatorname{range}}
\newcommand{\Img}{\operatorname{img}}
\newcommand{\Dim}{\operatorname{dim}}
\newcommand{\Eig}{\operatorname{Eig}}

%\tcbuselibrary{skins}
%\usetikzlibrary{shadings}


%%%
% Set up the margins to use a fairly large area of the page
%%%
\textwidth=5.5in
\topmargin=-1in
\textheight=10in
\parskip=.07in
\parindent=0in


\fancypagestyle{siefken}{%
	\rfoot{\footnotesize\it \copyright\,Jason Siefken, 2015--2018 \ \makebox(30,5){\includegraphics[height=1.2em]{by-sa.pdf}}}
	\lfoot{}
	\renewcommand{\headrulewidth}{0pt}
}
\fancypagestyle{iola}{%
	\rfoot{\footnotesize\it \copyright\,IOLA Team \url{iola.math.vt.edu} \ \makebox(30,5){\includegraphics[height=2.2em]{images/iolalogo.png}}}
	\lfoot{}
	\renewcommand{\headrulewidth}{0pt}
}



\begin{document}
\pagestyle{empty}

\begin{center}
{\huge\bf Inquiry Based Linear Algebra}\\

\vspace{.7in}
{
\it \copyright\,Jason Siefken, 2016--2017 \\
Creative Commons By-Attribution Share-Alike\, \makebox(30,5){\includegraphics[height=1.2em]{by-sa.pdf}}
}
\end{center}

\section*{About the Document}

	This document is a hybrid of many linear algebra resources, including those of
	the IOLA (Inquiry Oriented Linear Algebra) project, Jason Siefken's IBLLinearAlgebra
	project, and Asaki, Camfield, Moon, and Snipes' Radiograph and Tomography project.

	This document is a mix of student projects, problem sets, and labs.
	A typical class day looks like:
	\begin{enumerate}
		\item {\bf Introduction by instructor.} This may involve giving a definition,
			a broader context for the day's topics, or answering questions.
		\item {\bf Students work on problems.} Students work individually or in pairs
			on the prescribed problem.  During this time the instructor moves around
			the room addressing questions that students may have and giving one-on-one
			coaching.
		\item {\bf Instructor intervention.} If most students have successfully solved the 
			problem, the instructor regroups the class by providing a concise 
			explanation so that everyone is ready to move to the next concept.  This
			is also time for the instructor to ensure that everyone has understood the
			main point of the exercise (since it is sometimes easy to do some computation
			while being oblivious to the larger context).

			If students are having trouble, the instructor can give hints to the group,
			and additional guidance to ensure the students don't get frustrated
			to the point of giving up.
		\item {\bf Repeat step 2.}
	\end{enumerate}

	Using this format, students are working (and happily so) most of the class.
	Further, they are especially primed to hear the insights of the instructor, 
	having already invested substantially into each problem.

	This problem-set is geared towards concepts instead of computation, though some problems
	focus on simple computation.

	{\bf License}  Unless otherwise mentioned, pages of this document are licensed under the Creative Commons
	By-Attribution Share-Alike License.  That means, you are free to use,
	copy, and modify this document provided that you provide attribution
	to the previous copyright holders and you release your derivative work 
	under the same license.  Full text of the license is at \url{http://creativecommons.org/licenses/by-sa/4.0/}

	If you modify this document, you may add your name to the copyright list.  Also,
	if you think your contributions would be helpful to others, consider making a pull
	requestion, or opening an \emph{issue} at 
	\url{https://github.com/siefkenj/IBLLinearAlegbra}

	Content from other sources is reproduced here with permission and retains the Author's copyright.
	Please see the footnote of each page to verify the copyright.


\newpage

\setcounter{page}{1}
\pagestyle{siefken}
\section*{Vectors}

\begin{minipage}{6.2in}
	\question
	\begin{tikzpicture}[>=latex,scale=4]
		\draw[style=help lines] (0,0) (2,2);

		\draw[->,thick,black] (0,0) -- (135:1) node [left] {$\mathbf a$};
		\draw[->,thick,black] (0,0) -- (0,1) node [left] {$\mathbf b$};
		\draw[->,thick,black] (0,0) -- (45:1) node [above] {$\mathbf c$};
		\draw[->,thick,black] (0,0) -- (1,0) node [above] {$\mathbf d$};
		\draw (0,0) node [below] {$\mathbf o$};
		
		\draw[thick,blue,fill] (1,2) circle [radius=.02] node [below right,black] {$p$};
		\draw[thick,blue] (2,1) circle [radius=.02] node [below right,black] {$q$};

	\end{tikzpicture}

	Notice that all arrows in this diagram are the same length.
	We will call this length a \emph{unit}.
	\begin{parts}
		\item Give directions from $\bf o$ to $p$ of
		the form ``Walk \ul units in the direction of arrow \ul, then 
		walk \ul units in the direction of arrow \ul.''

		\item Can you give directions with the two arrows you haven't
		used?  Give such directions, or explain why it cannot be done.

		\item Give directions from $\bf o$ to $q$.

		\item Can you give directions from $\bf o$ to $q$ using $\bf c$ and $\bf a$?
		Give such directions, or explain why it cannot be done.

	\end{parts}

	\question
	\begin{minipage}{.35\textwidth}
	\begin{tikzpicture}[>=latex,scale=4]
		\draw[style=help lines] (0,0) (1,2);

		\draw[->,thick,black] (0,0) -- (0,1) node [left] {$\yh$};
		\draw[->,thick,black] (0,0) -- (63.4:1) node [left] {$\mathbf c$};
		\draw[->,thick,black] (0,0) -- (1,0) node [above] {$\xh$};
		\draw (0,0) node [below] {$\mathbf o$};
		
		\draw[thick,blue,fill] (1,2) circle [radius=.02] node [below right,black] {$p$};

	\end{tikzpicture}
	\end{minipage}
	\begin{minipage}{.65\textwidth}
	We are going to start using a more mathematical notation
	for giving directions.  Our directions will now look like
	\[
		p = \ull\, \xh + \ull\, \yh
	\]
	which is read as ``To get to $p$ (=) go \ul units in the direction $\xh$ then (+) go \ul units in 
	the direction $\yh$.''

	\begin{parts}
		\item What is the difference between $p = \ull\, \xh + \ull\, \yh$ and $p = \ull\, \yh + \ull\, \xh$?
		Can they both give valid directions?
		\item
		\begin{enumerate}
			\item Give directions to $p$ using the new notation. 
			\item Give directions to $p$ using $\bf c$.  (Notice that
				$\bf c$ is of unit length and points directly at $p$.)
			\item What is the distance from $\bf o$ to $p$ in units?
		\end{enumerate}
		\item
		\begin{enumerate}
			\item $r=1\bf c$.  Give directions from $\bf o$ to $r$ using $\xh$ and $\yh$.
			\item What is the distance from $\bf o$ to $r$?
		\end{enumerate}
		\item
		\begin{enumerate}
			\item $q=-2\xh+3\yh$; find the exact distance from $\bf o$ to $q$.
			\item $s=2\xh+\bf c$; find the exact distance from $\bf o$ to $s$.
		\end{enumerate}
	\end{parts}
	\end{minipage}


	The vectors $\xh$ and $\yh$ are called the \emph{standard
	basis vectors} for $\R^2$ (the plane).  
\end{minipage}

\section*{Column Vector Notation}
	We previously wrote $q=-2\xh+3\yh$.  In column vector notation we write
	\[
		q=\begin{bmatrix}-2\\3\end{bmatrix}
	\]
	We may call $q$ either a \emph{vector} or a \emph{point}.  If we call $q$ a vector,
	we are emphasizing that $q$ gives direction of some sort.  If we call $q$ a point,
	we emphasize that $q$ is some absolute location in space. (What's the philosophical
	difference between a location in space and directions from the origin to said location?)

	\question 
	$r=1\bf c$ and $s=2\xh+\bf c$ where $\bf c$ is the vector from before.
	\begin{parts}
		\item Write $r$ and $s$ in column vector form.
	\end{parts}

\section*{Sets and Set Notation}
\vspace{-.5cm}

	\begin{definition}[Set]
		A \emph{set} is a (possibly infinite) collection of items
		and is notated with curly braces (for example, $\{1,2,3\}$ is
		the set containing the numbers 1, 2, and 3).  We call the items in
		a set \emph{elements}.

		If $X$ is a set and $a$ is an elemento of $X$, we may write $a\in X$,
		which is read ``$a$ is an element of $X$.''

		If $X$ is a set, a \emph{subset} $Y$ of $X$ (written $Y\subseteq X$)
		is a set such that every element of $Y$ is an element of $X$.

		We can define a subset using \emph{set-builder notation}.
		That is, if $X$ is a set, we can define the subset 
		\[
			Y= \{a\in X:\text{some rule involving }a\},
		\]
		which is read ``$Y$ is the set of $a$ in $X$ {\bf such that} some rule
		involving $a$ is true.''  If $X$ is intuitive, we may omit it and
		simply write $Y=\{a:\text{some rule involving }a\}$.  You may equivalently
		use ``$|$'' instead of ``$:$'', writing $Y=\{a\,|\,\text{some rule involving }a\}$.
	\end{definition}

	\begin{definition}
		Some common sets are
		\begin{itemize}
			\item[] $\N=\{\text{natural numbers}\} = \{\text{non-negative whole numbers}\}$.
			\item[] $\Z=\{\text{integers}\} = \{\text{whole numbers, including negatives}\}$.
			\item[] $\R=\{\text{real numbers}\}$.
			\item[] $\R^n=\{\text{vectors in $n$-dimensional Euclidean space}\}$.
		\end{itemize}
	\end{definition}


	\question
	\begin{parts}
		\item Which of the following are true?
		\begin{enumerate}
			\item $3\in\{1,2,3\}$.
			\item $1.5\in\{1,2,3\}$.
			\item $4\in\{1,2,3\}$.
			\item ``b''$\in\{x:x\text{ is an English letter}\}$.
			\item ``\`o''$\in\{x:x\text{ is an English letter}\}$.
			\item $\{1,2\}\subseteq \{1,2,3\}$.
			\item For some $a\in\{1,2,3\}$, $a \geq 3$.
			\item For any $a\in\{1,2,3\}$, $a\geq 3$.
			\item $1\subseteq\{1,2,3\}$.
			\item $\{1,2,3\}=\{x\in\R:1\leq x\leq 3\}$.
			\item $\{1,2,3\}=\{x\in\Z:1\leq x\leq 3\}$.
		\end{enumerate}
	\end{parts}

	\question
		Write the following in set-builder notation
	\begin{parts}
			\item The subset $A\subseteq \R$ of real numbers larger than $\sqrt{2}$.
			\item The subset $B\subseteq \R^2$ of vectors whose first coordinate
			is twice the second.
	\end{parts}

	\begin{definition}[Unions \& Intersections]
		Two common set operations are \emph{unions} and \emph{intersections}.  
		Let $X$ and $Y$ be sets.

		\hfill\begin{minipage}{\dimexpr\textwidth-3cm}
		\begin{itemize}
			\item[(union)] $X\cup Y = \{a:a\in X\text{ or }a\in Y\}$.
			\item[(intersection)] $X\cap Y = \{a: a\in X\text{ and }a\in Y\}$.
		\end{itemize}
		\end{minipage}
	\end{definition}

	\question
	Let $X=\{1,2,3\}$ and $Y=\{2,3,4,5\}$ and $Z=\{4,5,6\}$.  Compute
	\begin{parts}
		\item $X\cup Y$
		\item $X\cap Y$
		\item $X\cup Y\cup Z$
		\item $X\cap Y\cap Z$
	\end{parts}

	\question
	Draw the following subsets of $\R^2$.
	\begin{parts}
		\item $V=\left\{\vec x\in\R^2:\vec x=\begin{bmatrix}0\\t\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $H=\left\{\vec x\in\R^2:\vec x=\begin{bmatrix}t\\0\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $J=\left\{\vec x\in\R^2:\vec x=t\begin{bmatrix}1\\1\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $V\cup H$.
		\item $V\cap H$.
		\item Does $V\cup H=\R^2$?
	\end{parts}



\section*{Systems of Linear Equations}
	
	\emph{Linear equations} are equations only involving variables, 
	multiplication by constants, and addition/subtraction.  \emph{Systems}
	of equations are sets of equations that share common variables.

	\question
	Consider the system
	\begin{equation}\label{eq2}
		\begin{array}{rcrl}
			x &-&y &= 2\\
			2x &+&y &= 1
		\end{array}
	\end{equation}

	\begin{parts}
		\item Draw the lines in (\ref{eq2}) on the same coordinate plane.
		\item Algebraically solve the system (\ref{eq2}).  What does this 
		solution represent on your graph?
	\end{parts}
	
	\question
	Let $L$ be the line given by $x-y=2$.
	\begin{parts}
		\item Write an equation of a line that doesn't intersect $L$.
		\item Write an equation of a line that intersects $L$ in 
		\begin{enumerate}
			\item one place.
			\item infinitely many places
			\item exactly two places
		\end{enumerate}
		or explain why no such equation exists.
		\item For each equation you came up with, solve the system algebraically.
		How can you tell algebraically how many solutions there are?
	\end{parts}

	\newpage
	\question
	Consider the augmented matrix
	\[
		A=\left[\begin{array}{ccc|c}
			1 & 2 & -1 & -7\\
			0 & 2 & 3 & 9\\
			0 & 0 & 1 & 1
		\end{array}\right].
	\]
	\begin{parts}
		\item Write the system of equations corresponding to $A$.
		\item Solve the system of equations corresponding to $A$.
	\end{parts}

\subsection*{The Row Reduction Algorithm}


	\question
	\begin{parts}
		\item Solve the system
		\begin{equation}\label{eq3}
			\begin{array}{rcrcrl}
				x&-&y&-&2z &= -5\\
				2x&+&3y&+&z &= 5\\
				0x&+&2y&+&3z &= 8
			\end{array}
		\end{equation}
		any way you like.

		\item Use an augmented matrix to solve the system (\ref{eq3}).
	\end{parts}

	The system (\ref{eq3}) can be interpreted in two ways (and switching between these 
	interpretations when appropriate is one of the most powerful tools of Linear 
	Algebra).  We can think of solutions to (\ref{eq3})
	as the intersection of three planes, or we can interpret the solution
	as coefficients of a linear combination.

	\begin{parts}[resume]
		\item Find $\vec v_1$, $\vec v_2$, $\vec v_3$, and $\vec p$ 
			so that you may rewrite (\ref{eq3}) as a vector equation of the form
		\[
			x\vec v_1+y\vec v_2+z\vec v_3 = \vec p
		\]
		where $x,y,z$ are interpreted as scalar quantities.

		\item If $(x,y,z)$ is a solution to (\ref{eq3}), explain how to get from the
		origin to $\vec p$ using only $\vec v_1, \vec v_2, \vec v_3$.
		\item If $(x,y,z)$ is a solution to \eqref{eq3}, is $\vec p$ a linear combination of $\vec v_1$,
			$\vec v_2$, and $\vec v_3$?
	\end{parts}
	
	\begin{definition}[Vector Form of a Line]
		A line $\ell$ is written in \emph{vector form} if it is expressed
		as
		\[
			\vec x=\vec d t+\vec p
		\]
		for some vector $\vec x$ and point $\vec p$. That is, $\ell = \{\vec x: \vec x=
		\vec d t+\vec p\text{ for some } t\in\R \}$.
	\end{definition}

\subsection*{Infinite Solutions}
	\question
	Consider the system
	\begin{equation}\label{eq4}
		\begin{array}{rcrl}
			x&+&2y &= 3\\
			2x&+&4y &= 6
		\end{array}
	\end{equation}

	\begin{parts}
		\item How many solutions does (\ref{eq4}) have?
		\item Write the solutions to (\ref{eq4}) in vector form.
		\item What happens when you use an augmented matrix
		to solve (\ref{eq4})?
	\end{parts}


\newpage
\subsection*{Free Variables}
	\question
	Suppose the row-reduced augmented matrix corresponding to 
	a system is
	\[
		B=\left[\begin{array}{cc|c}
			1 & 2 & 3\\
			0 & 0 & 0
		\end{array}\right].
	\]
	After reducing, we have 1 equation and 2 unknowns, so we can make
	$2-1=1$ choices when writing a solution.  Let's make the
	choice $y=t$.
	
	\begin{parts}
		\item With the added equation $y=t$, solve the
		system represented by $B$.
	\end{parts}

	\question
	Consider the system given by the augmented matrix
	\[
		C=\left[\begin{array}{ccccc|c}
			1&0&1&2&0&-1\\
			0&1&1&0&0&3\\
			0&0&0&0&1&4
		\end{array}\right].
	\]
	and call the variables in this system $x_1,x_2,
	x_3,x_4,x_5$.

	\begin{parts}
		\item Write the system of equations represented by $C$.
		\item Identify how many choices you can make when writing
		down a solution corresponding to $C$.
		\item Add one equation (of the form $x_i=t$ or $x_j=s$, etc.)
		for each choice you must make when solving the system.
		\item Write in vector form all solutions to $C$.
	\end{parts}

	\question
	\begin{parts}
		\item An unknown system $U$ is represented by an augmented
			matrix with 4 rows, 7 columns (one column is
			the augmented column).  What is 
			the minimum number of
			free variables $U$ can have?
		\item An unknown system $V$ is represented by an augmented
			matrix with 6 rows, 5 columns (one column is the
			augmented column).  What is 
			the minimum number of
			free variables  $V$ can have?
	\end{parts}
	
	\question
	\begin{definition}[Homogeneous]
		A system is called \emph{homogeneous} if all equations equal $0$.
	\end{definition}

		Let $A$ be an unknown system of $3$ equations and $3$ variables and suppose
		 $(x,y,z)=(1,2,1)$ and
		$(x,y,z)=(-1,1,1)$ are solutions to $A$.
	\begin{parts}
		\item Can you produce another solution
		to the system?

		\item  Can you
		produce a solution to the homogeneous version of $A$ (the version of $A$ where every
		equation equals 0)?

		\item Suppose when you use an augmented matrix to solve the system $A$, you only have 
		one free variable.  Could $A$ be homogeneous?  Can you produce all solutions to the system $A$?
	\end{parts}

\newpage
\section*{Dot Product}

	\begin{definition}[Dot Product]
	If $\vec a=\begin{bmatrix}a_1\\ a_2\\ \vdots \\ a_n\end{bmatrix}$ and 
	$\vec b=\begin{bmatrix}b_1\\ b_2\\ \vdots \\ b_n\end{bmatrix}$ are two vectors in $n$-dimensional
		space, then the \emph{dot product} of $\vec a$ an $\vec b$ is
	\[
		\vec a\cdot\vec b = a_1b_1+a_2b_2+\cdots+a_nb_n.
	\]
	Equivalently, the dot product is defined by the geometric formula
	\[
		\vec a\cdot \vec b = \|\vec a\|\|\vec b\|\cos \theta
	\]
	where $\theta$ is the angle between $\vec a$ and $\vec b$.
	\end{definition}
	
	\question
		Let $\vec a=\begin{bmatrix}1\\1\end{bmatrix}$, $\vec b=\begin{bmatrix}3\\2\end{bmatrix}$, and $\vec u=\begin{bmatrix}1\\2\\1\end{bmatrix}$.
	\begin{parts}
		\item 
		\begin{enumerate}	
			\item Draw a picture of $\vec a $ and $\vec b$.
			\item Compute $\vec a\cdot \vec b$.
			\item Find $\|\vec a\|$ and $\|\vec b\|$ and use your knowledge of
			the multiple ways to compute the dot product to find $\theta$,
			the angle between $\vec a$ and $\vec b$. Label $\theta$ on your picture.
		\end{enumerate}
		\item Draw the graph of $\cos$ and identify which angles make $\cos$ negative, zero,
		or positive.

		\item Draw a new picture of $\vec a$ and $\vec b$ and on that picture draw
		\begin{enumerate}	
			\item a vector $\vec c$ where $\vec c\cdot \vec a$ is negative.
			\item a vector $\vec d$ where $\vec d\cdot \vec a=0$ and $\vec d\cdot \vec b < 0$.
			\item a vector $\vec e$ where $\vec e\cdot \vec a=0$ and $\vec e\cdot \vec b>0$.
			\item Could you find a vector $\vec f$ where $\vec f\cdot \vec a=0$ and $\vec f\cdot \vec b=0$?
			Explain why or why not.
		\end{enumerate}

		\item Recall the vector $\vec u$ whose coordinates are given at the beginning of this problem.
		\begin{enumerate}
			\item Write down a vector $\vec v$ so that the angle between $\vec u$ and $\vec v$
			is $\pi/2$. (Hint, how does this relate to the dot product?)
			\item Write down another vector $\vec w$ (in a different direction from $\vec v$)
			so that the angle between $\vec w$ and $\vec u$ is $\pi/2$.
			\item Can you write down other vectors different than both $\vec v$ and $\vec w$ that still
			form an angle of $\pi/2$ with $\vec u$?  How many such vectors are there?
		\end{enumerate}
	\end{parts}

	\newpage
	\begin{definition}[Norm]
		The \emph{norm} of a vector $\vec v\in\R^n$, denoted $\|\vec v\|$ is its length
		and is given by the formula
		\[
			\|\vec v\| = \sqrt{\vec v\cdot\vec v}.
		\]
	\end{definition}

	\question
	\begin{parts}
		\item Let $\vec a = \mat{3\\2}$.  Find $\|\vec a\|$ using the Pythagorean theorem
			and using the formula from the definition of the norm.  How do
			these quantities relate?
		\item Let $\vec b = \mat{1\\1\\-2\\2}$, and find $\|\vec b\|$.
		Did you know how to find 4-d lengths before?
	
		\item Suppose $\vec u=\mat{x\\ y}$ for some $x,y\in \R$.
		Could $\vec u\cdot \vec u$ be negative? Compute $\vec u\cdot \vec u$ algebraically
		and use this to prove your answer.
	\end{parts}

	\begin{definition}[Distance]
		The \emph{distance} between two vectors $\vec u$ and $\vec v$ is $\|\vec u-\vec v\|$.
	\end{definition}
	\begin{definition}[Unit Vector]
		A vector $\vec v$ is called a \emph{unit vector} if $\|\vec v\|=1$.
	\end{definition}
	
	\question
	Let $\vec u=\mat{1\\2\\1}$ and $\vec v=\mat{1\\1\\3}$.
	\begin{parts}
		\item Find the distance between $\vec u$ and $\vec v$.
		\item Find a unit vector in the direction of $\vec u$.
		\item Does there exists a \emph{unit vector} $\vec x$ that is distance
			$1$ from $\vec u$?
		\item Suppose $\vec y$ is a unit vector and the distance between $\vec y$ and
			$\vec u$ is $2$.  What is the angle between $\vec y$ and $\vec u$?
	\end{parts}

	\begin{definition}[Orthogonal]
		Two vectors $\vec u$ and $\vec v$ are \emph{orthogonal} to each other
		if $\vec u\cdot \vec v=0$.  The word orthogonal is synonymous with the
		word perpendicular.
	\end{definition}

	\question
	\begin{parts}
		\item Find two vectors orthogonal to $\vec a=\mat{1\\-3}$.  Can you find two such vectors that
			are not parallel?
		\item Find two vectors orthogonal to $\vec b=\mat{1\\-3\\4}$.  Can you find two 
			such vectors that are not parallel?
		\item Suppose $\vec x$ and $\vec y$ are orthogonal to each other and $\|\vec x\|=5$ and $\|\vec y\|=3$.
			What is the distance between $\vec x$ and $\vec y$?
	\end{parts}


\newpage
\section*{Projections}
	Projections (sometimes called orthogonal projections) are a way to measure how much one vector
	points in the direction of another.

	\begin{definition}[Projection]
	\begin{center}
	\usetikzlibrary{patterns,decorations.pathreplacing}
	\begin{tikzpicture}[>=latex,scale=2.5]
		\draw[->,thick,black] (0,0) -- (2,1) node [above] {$\vec u$};
		\draw[->,thick,black] (0,0) -- (3,0) node [above] {$\vec v$};
		\draw[->,thick,black,yshift=-.07cm] (0,0) -- (2,0);
		\draw[decoration={brace, mirror}, decorate, yshift=-.15cm] (0,0) -- (2,0) node [midway,below,yshift=-4pt] {$\Proj_{\vec v}\vec u$};
		
		\draw[dashed,thick,black] (2,0) -- (2,1);
		\draw[decoration={brace, mirror}, decorate, xshift=1.15cm] (2,0) -- (2,1) node [midway,right,xshift=4pt] {$\Perp_{\vec v}\vec u$};
		\draw[thin,black] (1.85,0)--(1.85,.15)--(2,.15);

	\end{tikzpicture}
	\end{center}
	\vspace{-.6cm}
	
	The \emph{projection} of $\vec u$ onto $\vec v$ is written $\proj_{\vec v}\vec u$ and is the vector in the direction of $\vec v$
	such that $\vec u-\Proj_{\vec v}\vec u$ is orthogonal to $\vec v$.  The vector $\vec u-\Proj_{\vec v}$ is called the \emph{perpendicular
	component} of $\vec u$ with respect to $\vec v$ and is notated as $\Perp_{\vec v}\vec u$.
	\end{definition}
	
	\question
	\begin{center}
	\usetikzlibrary{patterns,decorations.pathreplacing}
	\begin{tikzpicture}[>=latex,scale=2.5]
		\draw[->,thick,black] (0,0) -- (30:2) node [above] {$\vec a$};
		\draw[->,thick,black] (0,0) -- (3,0) node [above] {$\vec b$};
		\draw[thick,black] (0,0) +(0:.3cm) arc (0:30:.3cm) node[right,yshift=-4pt] {$\theta$};

	\end{tikzpicture}
	\end{center}


	In this picture $\|\vec a\|=4$, $\theta = \pi/6$, and $\vec b=\mat{6\\0}$.
	%%%
	% XXX: This is confusing with multiple b vectors. Should instead emphasise that proj_b(a ) = proj_{-b}(a).
	%%%
	\begin{parts}
		\item   Write $\vec a$ in column vector form.
		\item   Find $\|\Proj_{\vec b}\vec a\|$ and $\|\Perp_{\vec b}\vec a\|$.
		\item Write down 
				$\Proj_{\vec b}\vec a$ and $\Perp_{\vec b}\vec a$ in column vector
				form.
		\item Let $\vec c=\mat{-4\\0}$. Write down 
				$\Proj_{\vec c}\vec a$ and $\Perp_{\vec c}\vec a$ in column vector
				form.
	\end{parts}

	\question
	Let $\|\vec a\|=4$, $\vec d=\mat{3\\1}$ and let $\theta = \pi/6$ be the angle between $\vec a$ and $\vec d$.
	\begin{parts}
		\item Write down 
				$\Proj_{\vec d}\vec a$ and $\Perp_{\vec d}\vec a$ in column vector
				form.
		\item Compute $\proj_{\xh}\vec d$ and 
		$\proj_{\yh}\vec d$.  How do these projections relate to the coordinates of $\vec d$? What
		can you say in general about projections onto $\xh$ and $\yh$?
	\end{parts}

\newpage
\subsection*{Matrix Equations}
Let $M$ be an $n\times m$ matrix with rows $\vec r_1,\ldots, \vec r_n$ and let $\vec v$ be a $1\times m$ vector.
Then
\[
	M\vec v=\mat{\vec r_1\cdot \vec v\\\vdots\\ \vec r_n\cdot \vec v}.
\]

	\question
	\begin{parts}
		\item Let $M=\mat{1&5&2\\2&1&1\\0&1&0}$ and $\vec v=\mat{8\\1\\1}$. Compute $M\vec v$.
	\end{parts}

	\question
		Consider the system
		\begin{equation}\label{eqmatvecprod}
			\begin{array}{llll}
				x&+2y&+z &= 1\\
				x&+2y&+3z &= 2\\
				-x&-2y&+z &= 3
			\end{array}
		\end{equation}
		\begin{parts}
			\item Write \eqref{eqmatvecprod} as a vector equation.
			\item Write \eqref{eqmatvecprod} as a matrix equation (i.e., one of the form $M\vec x=\vec b$).
		\end{parts}

	\question
	Let $M$ be an $n\times m$ matrix with columns $\vec c_1,\ldots \vec c_m$ and rows $\vec r_1,\ldots,\vec r_n$.
	Let $\vec x=\mat{x_1\\\vdots\\ x_m}$.
	\begin{parts}
		\item Express $M\vec x$ in terms of $\vec r_1,\ldots,\vec r_n$ and $\vec x$.
		\item Express $M\vec x$ in terms of $\vec c_1,\ldots,\vec c_m$ and $\vec x$.
	\end{parts}





\section*{Linear Combinations, Span, and Linear Independence}
	\vspace{-1em}

	\begin{definition}[Linear Combination]
		A \emph{linear combination} of the vectors $\vec v_1,\vec v_2,\ldots,\vec v_n$ is
		a vector
		\[
			\vec w = \alpha_1\vec v_1+\alpha_2\vec v_2+\cdots+\alpha_n\vec v_n
		\]
		where $\alpha_1,\alpha_2,\ldots,\alpha_n$ are scalars.
	\end{definition}

	\question
	Let $\vec v_1=\begin{bmatrix}1\\1\end{bmatrix}$, $\vec v_2=\begin{bmatrix}1\\-1\end{bmatrix}$, and $\vec w=2\vec v_1+\vec v_2$.
	\begin{parts}
		\item Write the coordinates of $\vec w$.
		\item Draw a picture with $\vec w$, $\vec v_1$, and $\vec v_2$.
		\item Is $\mat{3\\3}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Is $\mat{0\\0}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Is $\mat{4\\0}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Can you find a vector in $\R^2$ that isn't a linear combination of
		$\vec v_1$ and $\vec v_2$?
		\item Can you find a vector in $\R^2$ that isn't a linear combination of
		$\vec v_1$?
	\end{parts}

	\begin{definition}[Span]
		The \emph{span} of a set of vectors $V$ is the set of
		all linear combinations of vectors in $V$.  That is,
		\[
			\Span V = \{\vec v:\vec v=\alpha_1\vec v_1+\alpha_2\vec v_2 + \cdots 
			+\alpha_n\vec v_n\text{ for some }\vec v_1,\vec v_2,\ldots,\vec v_n\in V
			\text{ and scalars }\alpha_1,\alpha_2,\ldots,\alpha_n\}.
		\]
	\end{definition}

	\vspace{-.3cm}
	\question
	Let $\vec v_1=\mat{1\\1}$, $\vec v_2=\mat{1\\-1}$, and $\vec v_3=\mat{2\\2}$.
	\begin{parts}
		\item Draw $\Span\{\vec v_1\}$.
		\item Draw $\Span\{\vec v_2\}$.
		\item Describe $\Span\{\vec v_1,\vec v_2\}$.
		\item Describe $\Span\{\vec v_1,\vec v_3\}$.
		\item Describe $\Span\{\vec v_1,\vec v_2,\vec v_3\}$.
	\end{parts}


	In some sets, every vector is essential for computing a span.  In others,
	there are ``excess'' vectors.  This leads us to the concept of 
	linear independence.

	\begin{definition}[Linearly Dependent \& Independent]
		We say $\{\vec v_1,\vec v_2,\ldots,\vec v_n\}$ is
		\emph{linearly dependent} if for at least one $i$,
		\[
			\vec v_i\in\span\{\vec v_1,\vec v_2,\ldots,\vec v_{i-1},
			\vec v_{i+1},\ldots,\vec v_n\},
		\]
		and a set is \emph{linearly independent} otherwise.
	\end{definition}

	\question
		Let $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, and $\vec w=\mat{1\\1\\0}$.
	\begin{parts}
		\item Describe $\Span\{\vec u,\vec v,\vec w\}$.
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?  Why or why not?
	\end{parts}

	Let $X=\{\vec u,\vec v,\vec w\}$.

	\begin{parts}[resume]
		\item Give a subset $Y\subseteq X$ so that $\Span Y=\Span X$ and $Y$ is
		linearly independent.
		\item Give a subset $Z\subseteq X$ so that $\Span Z=\Span X$ and $Z$ is
		linearly independent and $Z\neq Y$.
	\end{parts}
	
\newpage
\pagestyle{iola}
\section*{Task 1.1: The Magic Carpet Ride}

You are a young traveler, leaving home for the first time. Your parents
want to help you on your journey, so just before your departure, they give
you two gifts. Specifically, they give you two forms of transportation:
a hover board and a magic carpet. Your parents inform you that both the
hover board and the magic carpet have restrictions in how they operate:


\begin{minipage}{\textwidth}
	\vspace{.5cm}
	\begin{wrapfigure}{l}{1in}
	\vspace{-.8cm}
	\includegraphics[width=1in]{images/HoverBoard-small.png}
	\end{wrapfigure}

	We denote the restriction on the hover board's movement by the vector
	$\mat{3 \\1}$. By this we mean that if
	the hover board traveled ``forward'' for one hour, it would move along a
	``diagonal'' path that would result in a displacement of 3 miles East and
	1 mile North of its starting location.
\end{minipage}

\begin{minipage}{\textwidth}
	\vspace{.5cm}
	\begin{wrapfigure}{l}{1in}
	\vspace{-.8cm}
	\includegraphics[width=1in]{images/MagicCarpet-small.png}
	\end{wrapfigure}

	We denote the restriction on the magic carpet's movement by the vector
	$\mat{1 \\2 }$. By this we mean that if the
	magic carpet traveled ``forward'' for one hour, it would move along a
	``diagonal'' path that would result in a displacement of 1 mile East and
	2 miles North of its starting location.
\end{minipage}

\lfoot{\footnotesize Drawings by \url{@DavidsonJohnR} (twitter)}

\vspace{10mm}

% Scenario Section
\textbf{Scenario One: The Maiden Voyage}

Your Uncle Cramer suggests that your first adventure should be to go visit
the wise man, Old Man Gauss. Uncle Cramer tells you that Old Man Gauss
lives in a cabin that is 107 miles East and 64 miles North of your home.

\vspace{5mm}

\textbf{Task:}
\par
Investigate whether or not you can use the hover board and the magic
carpet to get to Gauss's cabin. If so, how? If it is not possible to
get to the cabin with these modes of transportation, why is that the case?

%\vspace{5mm}
% As a group, state and explain your answer(s) on the group whiteboard. Use
% the vector notation for each mode of transportation as part of your
% explanation and use a diagram or graphic to help illustrate your
% point(s).

\newpage
\section*{Task 1.2: The Magic Carpet Ride, Hide and Seek}


You are a young traveler, leaving home for the first time. Your parents
want to help you on your journey, so just before your departure, they give
you two gifts. Specifically, they give you two forms of transportation:
a hover board and a magic carpet. Your parents inform you that both the
hover board and the magic carpet have restrictions in how they operate:



\begin{minipage}{\textwidth}
	\vspace{.5cm}
	\begin{wrapfigure}{l}{1in}
	\vspace{-.8cm}
	\includegraphics[width=1in]{images/HoverBoard-small.png}
	\end{wrapfigure}

	We denote the restriction on the hover board's movement by the vector
	$\mat{3 \\1}$. By this we mean that if
	the hover board traveled ``forward'' for one hour, it would move along a
	``diagonal'' path that would result in a displacement of 3 miles East and
	1 mile North of its starting location.
\end{minipage}

\begin{minipage}{\textwidth}
	\vspace{.5cm}
	\begin{wrapfigure}{l}{1in}
	\vspace{-.8cm}
	\includegraphics[width=1in]{images/MagicCarpet-small.png}
	\end{wrapfigure}

	We denote the restriction on the magic carpet's movement by the vector
	$\mat{1 \\2 }$. By this we mean that if the
	magic carpet traveled ``forward'' for one hour, it would move along a
	``diagonal'' path that would result in a displacement of 1 mile East and
	2 miles North of its starting location.
	\vspace{1cm}
\end{minipage}



\textbf{Scenario Two: Hide-and-Seek}

Old Man Gauss wants to move to a cabin in a different location. You are
not sure whether Gauss is just trying to test your wits at finding him
or if he actually wants to hide somewhere that you can't visit him.

\vspace{5mm}

\textbf{Are there some locations that he can hide and you cannot reach him
with these two modes of transportation?}

Describe the places that you
can reach using a combination of the hover board and the magic carpet and
those you cannot. Specify these geometrically and algebraically. Include
a symbolic representation using vector notation. Also, include a convincing
argument supporting your answer.

%\vspace{5mm} \par \textbf{Use your
%group's whiteboard as a space to write out our work as your work together
%on this problem.}




\newpage
\pagestyle{siefken}

\newpage
\pagestyle{iola}
\section*{Task 1.3: The Magic Carpet, Getting Back Home}

Suppose you are now in a three-dimensional world for the carpet
ride problem, and you have three modes of transportation:
\[
	\vec v_1 = \mat{1 \\1 \\ 1}\qquad
	\vec v_2 = \mat{6 \\3 \\ 8}\qquad
	\vec v_3 = \mat{4 \\1 \\ 6}
\]

You are only allowed to use each mode of transportation \textbf{once}
(in the forward or backward direction) for a fixed amount of time ($c_1$
on $\vec v_1$, $c_2$ on $\vec v_2$, $c_3$ on $\vec v_3$). Find the amounts of time on
each mode of transportation ($c_1$, $c_2$,  and $c_3$, respectively)
needed to go on a journey that starts and ends at home \emph{or} explain why
it is not possible to do so.

\vspace{10mm}

\newpage

\begin{enumerate}
	\item Is there more than one way to make a journey that meets the
	requirements described above? (In other words, are there different
	combinations of times you can spend on the modes of transportation so
	that you can get back home?) If so, how?

	\vspace{65mm} 
	\item Is there anywhere in this 3D world that Gauss
	  could hide from you? If so, where? If not, why not?

	\vspace{65mm}
	\item What is $\Span \left\{\mat{1 \\1 \\  1},\mat{6 \\3 \\ 8},\mat{4  \\1 \\ 6} \right\}$?

\end{enumerate}


\newpage
\pagestyle{siefken}




	\begin{definition}[Trivial Linear Combination]
	We say a linear combination 
	$a_1\vec v_1+a_2\vec v_2+\cdots +a_n\vec v_n$
	is \emph{trivial} if $a_1=a_2=\cdots=a_n=0$.
	\end{definition}
	
	\question
		Recall $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, and $\vec w=\mat{1\\1\\0}$.
	\begin{parts}
		\item Consider the linearly dependent 
		set $\{\vec u,\vec v,\vec w\}$ (where $\vec u,\vec v,\vec w$
		are defined as above).  Can you write $\vec 0$
		as a non-trivial linear combination of vectors in this set?
		\item Consider the linearly independent 
		set $\{\vec u,\vec v\}$.  Can you write $\vec 0$
		as a non-trivial linear combination of vectors in this set?
	\end{parts}

	We now have an equivalent definition of linear dependence.

	\begin{definition}[Linearly Dependent \& Independent]
	$\{\vec v_1,\vec v_2,\ldots,\vec v_n\}$ is
	\emph{linearly dependent} if there is a non-trivial
	linear combination of $\vec v_1,\ldots,\vec v_n$ that
	equals the zero vector.
	\end{definition}

	\question
	\begin{parts}
		\item Explain how this new definition implies the old one.
		\item Explain how the old definition implies this new one.
	\end{parts}

	Since we have old def $\implies$ new def, and new def $\implies$ old def ($\implies$
	should be read aloud as `implies'), the two definitions
	are \emph{equivalent} (which we write as new def $\iff$ old def).


	\question
	Suppose for some unknown $\vec u, \vec v, \vec w$, and $\vec a$,
	\[
		\vec a = 3\vec u+2\vec v +\vec w\qquad \text{and}\qquad 
		\vec a = 2\vec u+\vec v -\vec w.
	\]
	\begin{parts}
		\item Could the set $\{\vec u,\vec v,\vec w\}$ be linearly
		independent?
	\end{parts}
	Suppose that
	\[
		\vec a = \vec u+6\vec r-\vec s
	\]
	is the \emph{only} way to write $\vec a$ using $\vec u,\vec r,\vec s$.
	\begin{parts}[resume]
		\item Is $\{\vec u,\vec r,\vec s\}$ linearly independent?
		\item Is $\{\vec u,\vec r\}$ linearly independent?
		\item Is $\{\vec u,\vec v,\vec w,\vec r\}$ linearly independent?
	\end{parts}

	\question
	Consider the system 
		\begin{equation}\label{eq4bc}
			\begin{array}{llll}
				x&-y&-z &= 0\\
				0x&+1y&+2z &= 0\\
				3x&-3y&+3z &= 0
			\end{array}
		\end{equation}
	which has the unique solution $(x,y,z)=(0,0,0)$.
	\begin{parts}
		\item Give vectors $\vec u,\vec v,\vec w$ so that the system \eqref{eq4bc}
			corresponds to the vector equation $x\vec u+y\vec v+z\vec w = \vec 0$.
		\item Is $\vec w\in\Span\{\vec u,\vec v\}$? If so, write it as a linear combination
			of $\vec u$ and $\vec v$.
	\end{parts}

	\newpage
	\question
	The matrix $M$ is the non-augmented matrix corresponding to a homogeneous system of linear equations.
	$M$ also corresponds to the vector equation $x\vec a+y\vec b+z\vec c=\vec 0$.  Further, we know
	\[
		\rref(M) = \mat{1&0&1\\0&1&-2\\0&0&0}.
	\]
	\begin{parts}
		\item Give a solution to the vector equation $x\vec a+y\vec b+z\vec c=\vec 0$.
		\item Is $\vec c\in\Span\{\vec a,\vec b\}$?  If so, write it as a linear combination
			of $\vec a$ and $\vec b$.
		\item Do you have enough information to tell if $\{\vec a,\vec b\}$ is linearly independent?  Why or why not?
	\end{parts}

\subsection*{Finding Linearly Independent Subsets}
	\question
	Suppose when you use an augmented matrix to solve
	$a\vec u+b\vec v+c\vec w=\vec 0$ you have no free variables.
	
	\begin{parts}
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?
	\end{parts}
	
	Suppose when you use an augmented matrix to solve
	$a\vec u+b\vec v+c\vec w=\vec 0$, the second column (and only the second column) corresponds to a 
	free variable.
	
	\begin{parts}[resume]
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?
		\item Is $\{\vec u,\vec w\}$ linearly independent?
		\item Is $\{\vec u,\vec v\}$ linearly independent?
	\end{parts}

	\begin{definition}[Maximal Linearly Independent Subset]
	Given a set of vectors $X$, a 
	\emph{maximal linearly independent subset} of $X$ is a linearly independent
	subset $V\subseteq X$ with the most possible vectors in it 
	(i.e., if you took any subset of $X$ with more vectors, it would be linearly
	dependent).
	\end{definition}

	\question
	\begin{parts}
		\item Give a maximal linearly independent subset, $T$, of
		$\left\{\mat{a\\b\\c}:a,b,c\in \R\right\}$.
		\item What is the size of $T$?
	\end{parts}

	\question
	Consider the vectors
	\[
		\vec v_1=\mat{1\\2\\1}
		\qquad
		\vec v_2=\mat{-1\\-1\\-1}
		\qquad
		\vec v_3=\mat{0\\1\\0}
		\qquad
		\vec v_4=\mat{-1\\2\\0}
		\qquad
		\vec v_5=\mat{1\\-1\\1}
	\]
	and the matrices
	\[
		A=\mat{1&-1&0&-1&1\\ 2&-1&1&2&-1\\1 & -1&0&0&1}
		\qquad \rref (A)
		=\mat{1&0&1&0&-2\\0&1&1&0&-3\\0&0&0&1&0}.
	\]
	(Notice that the columns of $A$ are the vectors $\vec v_1,\ldots, \vec v_5$)

	\begin{parts}
		\item Is $V=\{\vec v_1,\vec v_2,\vec v_3,\vec v_4,\vec v_5\}$ linearly
		independent?
		\item Pick a maximal linearly independent subset of $V$.
		\item Pick another (different) maximal linearly independent subset of $V$.
%		\item Give a basis for $\Span(V)$.
%		\item What is the dimension of $\Span(V)$?
	\end{parts}


\newpage
\pagestyle{iola}
\section*{Task 2.1: Italicizing N}

\hfill\begin{tikzpicture}[scale=1.5]
    \begin{axis}[
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-.5,
		 xmax=3.5,
		 ymin=-.5,
		 ymax=5.5,
		 major grid style={dotted, gray, thick},
                 xtick={0,1,2,3},
		 ytick={0,1,2,3,4,5},
                 grid=both]

	 \draw[black, thick] (0,0) -- (0,3) -- (2,0) -- (2,3);
    \end{axis}
\end{tikzpicture}\hfill
\begin{tikzpicture}[scale=1.5]
    \begin{axis}[
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-.5,
		 xmax=3.5,
		 ymin=-.5,
		 ymax=5.5,
		 major grid style={dotted, gray, thick},
                 xtick={0,1,2,3},
		 ytick={0,1,2,3,4,5},
                 grid=both]

	 \draw[black, thick] (0,0) -- (1,4) -- (2,0) -- (3,4);
    \end{axis}
\end{tikzpicture}\hfill

Suppose that the ``N'' on the left is written in regular 12-point font.  Find a matrix $A$ that will transform
	the ``N'' into the letter on the right which is written in an \emph{italic} 16-point font.

Work with your group to write out your solution and approach.  Make a list of any assumptions you
notice your group making or any questions for further pursuit.

\newpage
\section*{Task 2.2: Beyond the N}

\begin{minipage}{0.4\textwidth}
\begin{tikzpicture}
    \begin{axis}[scale=1.2,
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-4.5,
		 xmax=7.5,
		 ymin=-4.5,
		 ymax=11.5,
		 major grid style={dotted, gray},
                 xtick={-10,-9,...,10},
                 ytick={-10,-9,...,10},
                 grid=both]
	 \draw (0,0) circle[radius=1pt,fill=blue] node[below right] {$(0,0)$};
	 \draw[black, thick] (0,0) -- (0,3) -- (2,0) -- (2,3);

	 \draw[black, thick] (0,0) -- (-2,0) -- (-2,-3) -- (0,-3)(-2,-1.5) -- (0,-1.5);

	\draw[black, thick] (5, 3) -- (3, 3) -- (3, 0) -- (5, 0)  (3, 1.5) -- (5, 1.5);
	    \draw[black, thick] (2, 8) -- (0, 8) -- (0, 5) -- (2, 5)  (0, 6.5) -- (2, 6.5);

    \end{axis}
\end{tikzpicture}
\end{minipage}
\begin{minipage}{.6\textwidth}
	A few students were wondering how letters placed in other
locations in the plane would be transformed under $A= \mat{ 1&  1/3  \\ 0 & 4/3}$. 
	If an ``E'' is placed around the
	``N,'' the students argued over four different possible results for the
	transformed E's. Which choice below, if any, is correct, and why? If none of
	the four options are correct, what would the correct option be, and why?
\end{minipage}

\vfill
\hfill
\begin{tikzpicture}
    \begin{axis}[scale=1.2,
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-4.5,
		 xmax=7.5,
		 ymin=-4.5,
		 ymax=11.5,
		 major grid style={dotted, gray},
                 xtick={-10,-9,...,10},
                 ytick={-10,-9,...,11},
                 grid=both]
	\draw (0,0) circle[radius=1pt,fill=blue] node[below right] {$(0,0)$};
	\draw[black, thick] (0, 0) -- (1, 4) -- (2, 0) -- (3, 4);

	\draw[black, thick] (-1, -4) -- (-3, -4) -- (-2, 0) -- (0, 0)  (-2.5, -2) -- (-0.5, -2);
	\draw[black, thick] (5, 0) -- (3, 0) -- (4, 4) -- (6, 4) (3.5, 2) -- (5.5, 2);
	\draw[black, thick](2, 6) -- (0, 6) -- (1, 10) -- (3, 10) (0.5, 8) -- (2.5, 8) ;
    \end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
    \begin{axis}[scale=1.2,
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-4.5,
		 xmax=7.5,
		 ymin=-4.5,
		 ymax=11.5,
		 major grid style={dotted, gray},
                 xtick={-10,-9,...,10},
                 ytick={-10,-9,...,11},
                 grid=both]
	\draw (0,0) circle[radius=1pt,fill=blue] node[below right] {$(0,0)$};
	\draw[black, thick] (0, 0) -- (1, 4) -- (2, 0) -- (3, 4);

	\draw[black, thick] (-1, -4) -- (-3, -4) -- (-2, 0) -- (0, 0)  (-2.5, -2) -- (-0.5, -2);
	\draw[black, thick] (5, 0) -- (3, 0) -- (4, 4) -- (6, 4) (3.5, 2) -- (5.5, 2);
	\draw[black, thick] (3.5, 6) -- (1.5, 6) -- (2.5, 10) -- (4.5, 10) (2.0, 8) -- (4.0, 8);
    \end{axis}
\end{tikzpicture}
\hfill

\hfill
\begin{tikzpicture}
    \begin{axis}[scale=1.2,
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-4.5,
		 xmax=7.5,
		 ymin=-4.5,
		 ymax=11.5,
		 major grid style={dotted, gray},
                 xtick={-10,-9,...,10},
                 ytick={-10,-9,...,11},
                 grid=both]
	\draw (0,0) circle[radius=1pt,fill=blue] node[below right] {$(0,0)$};
	\draw[black, thick] (0, 0) -- (1, 4) -- (2, 0) -- (3, 4);

	\draw[black, thick] (-1, -4) -- (-3, -4) -- (-2, 0) -- (0, 0)  (-2.5, -2) -- (-0.5, -2);
	\draw[black, thick] (5, 0) -- (3, 0) -- (4, 4) -- (6, 4) (3.5, 2) -- (5.5, 2);
	\draw[black, thick] (3.666666666666667, 6.666666666666667) -- (1.6666666666666667, 6.666666666666667) -- (2.666666666666667, 10.666666666666668) -- (4.666666666666667, 10.666666666666668)  (2.166666666666667, 8.666666666666668) -- (4.166666666666667, 8.666666666666668);
    \end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
    \begin{axis}[scale=1.2,
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-4.5,
		 xmax=7.5,
		 ymin=-4.5,
		 ymax=11.5,
		 major grid style={dotted, gray},
                 xtick={-10,-9,...,10},
                 ytick={-10,-9,...,11},
                 grid=both]
	\draw (0,0) circle[radius=1pt,fill=blue] node[below right] {$(0,0)$};
	\draw[black, thick] (0, 0) -- (1, 4) -- (2, 0) -- (3, 4);

	\draw[black, thick] (0, -4) -- (-2, -4) -- (-1, 0) -- (1, 0)  (-1.5, -2) -- (0.5, -2);
	\draw[black, thick] (5, 0) -- (3, 0) -- (4, 4) -- (6, 4) (3.5, 2) -- (5.5, 2);
	\draw[black, thick] (3.666666666666667, 6.666666666666667) -- (1.6666666666666667, 6.666666666666667) -- (2.666666666666667, 10.666666666666668) -- (4.666666666666667, 10.666666666666668)  (2.166666666666667, 8.666666666666668) -- (4.166666666666667, 8.666666666666668);
    \end{axis}
\end{tikzpicture}
\hfill
\vfill

\newpage

\section*{Task 2.3: Pat and Jamie}

\begin{minipage}{.55\textwidth}
\begin{tikzpicture}[scale=.9]
    \begin{axis}[
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-.5,
		 xmax=3.5,
		 ymin=-.5,
		 ymax=5.5,
		 major grid style={dotted, gray, thick},
                 xtick={0,1,2,3},
		 ytick={0,1,2,3,4,5},
                 grid=both]

	 \draw[black, thick] (0,0) -- (0,3) -- (2,0) -- (2,3);
    \end{axis}
\end{tikzpicture}\hfill
\begin{tikzpicture}[scale=.9]
    \begin{axis}[
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-.5,
		 xmax=3.5,
		 ymin=-.5,
		 ymax=5.5,
		 major grid style={dotted, gray, thick},
                 xtick={0,1,2,3},
		 ytick={0,1,2,3,4,5},
                 grid=both]

	 \draw[black, thick] (0,0) -- (1,4) -- (2,0) -- (3,4);
    \end{axis}
\end{tikzpicture}\hfill
\end{minipage}
\begin{minipage}{.45\textwidth}

Suppose that the ``N'' on the left is written in regular 12-point font.  Find a matrix $A$ that will transform
	the ``N'' into the letter on the right which is written in an \emph{italic} 16-point font.
\end{minipage}

Two students---Pat and Jamie---explained their approach to the Italicizing N task as follows:
\begin{quote}\itshape
	In order to find the matrix $A$, we are going to find a matrix that makes the ``N'' taller,
	find a matrix that italicizes the taller ``N,'' and a combination of those two matrices
	will give the desired matrix $A$.
\end{quote}

\begin{enumerate}
	\item Do you think Pat and Jamie's approach allowed them to find $A$?  If so, do
		you think they found the same matrix that you did during Italicising N?
		\vspace{\stretch{1}}
	\item Try Pat and Jamie's approach.  Either (a) come up with a matrix $A$ using
		their approach, or (b) explain why their approach does not work.
		\vspace{\stretch{2}}
\end{enumerate}



\newpage
\section*{Task 2.4: Getting back N}

\begin{minipage}{.55\textwidth}
\begin{tikzpicture}[scale=.9]
    \begin{axis}[
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-.5,
		 xmax=3.5,
		 ymin=-.5,
		 ymax=5.5,
		 major grid style={dotted, gray, thick},
                 xtick={0,1,2,3},
		 ytick={0,1,2,3,4,5},
                 grid=both]

	 \draw[black, thick] (0,0) -- (0,3) -- (2,0) -- (2,3);
    \end{axis}
\end{tikzpicture}\hfill
\begin{tikzpicture}[scale=.9]
    \begin{axis}[
		    axis equal image,
		    axis line style={draw=none},
		    tick style={draw=none},
		    yticklabels={,,},
		    xticklabels={,,},
		 xmin=-.5,
		 xmax=3.5,
		 ymin=-.5,
		 ymax=5.5,
		 major grid style={dotted, gray, thick},
                 xtick={0,1,2,3},
		 ytick={0,1,2,3,4,5},
                 grid=both]

	 \draw[black, thick] (0,0) -- (1,4) -- (2,0) -- (3,4);
    \end{axis}
\end{tikzpicture}\hfill
\end{minipage}
\begin{minipage}{.45\textwidth}

Suppose that the ``N'' on the left is written in regular 12-point font.  Find a matrix $A$ that will transform
	the ``N'' into the letter on the right which is written in an \emph{italic} 16-point font.
\end{minipage}

Two students---Pat and Jamie---explained their approach to the Italicizing N task as follows:
\begin{quote}\itshape
	In order to find the matrix $A$, we are going to find a matrix that makes the ``N'' taller,
	find a matrix that italicizes the taller ``N,'' and a combination of those two matrices
	will give the desired matrix $A$.
\end{quote}

Consider the new task: find a matrix $C$ that transforms the ``N'' on the right to
the ``N'' on the left.
\begin{enumerate}
	\item Use any method you like to find $C$.
		\vspace{\stretch{1}}
	\item Use a method similar to Pat and Jamie's method, only use it to find $C$ instead
		of $A$.
		\vspace{\stretch{1}}
\end{enumerate}















\newpage
\pagestyle{siefken}


\section*{Linear Transformations}
\vspace{-1.5em}
	
	\question
	$\mathcal R:\R^2\to\R^2$ is the transformation that rotates vectors counter-clockwise 
	by $90^\circ$.
	\begin{parts}
		\item Compute $\mathcal R\mat{1\\0}$ and $\mathcal R\mat{0\\1}$.
		\item Compute $\mathcal R\mat{1\\1}$.  How does this relate to
			$\mathcal R\mat{1\\0}$ and $\mathcal R\mat{0\\1}$?
		\item What is $\mathcal R\left(a\mat{1\\0}+b\mat{0\\1}\right)$?
		\item Write down a matrix $R$ so that $R\vec v$ is $\vec v$ rotated
			counter clockwise by $90^\circ$.
	\end{parts}

	\question
	$\mathcal S:\R^3\to\R^3$ stretches in the $\zh$ direction  by a factor of $2$
	and contracts in the $\yh$ direction by a factor of $3$.
	\begin{parts}
		\item Write a matrix representation of $\mathcal S$.
	\end{parts}

	\begin{definition}[Linear Transformation]
		If $V$ and $W$ are vector spaces, a function $T:V\to W$ is called a \emph{linear transformation}
		if 
		\[
			T(\vec u+\vec v)=T\vec u+T\vec v \qquad\text{and}\qquad
			T(\alpha \vec v)=\alpha T\vec v
		\]
		for all vectors $\vec u,\vec v\in V$ and all scalars $\alpha$.
	\end{definition}

	\question
	\begin{parts}
		\item Classify the following as linear transformations or not
			\begin{enumerate}
				\item $\mathcal R$ from before.
				\item $\mathcal S$ from before.
				\item $W:\R^2\to\R^2$ where $W\mat{x\\y}=\mat{x^2\\y}$.
				\item $T:\R^2\to\R^2$ where $T\mat{x\\y}=\mat{x+2\\y}$.
				\item $\mathcal P:\R^2\to\R^2$ where $\mathcal P\mat{x\\y}=\proj_{\vec u}\mat{x\\y}$ and 
					$\vec u=\mat{2\\3}$.
			\end{enumerate}
	\end{parts}

	It turns out every (finite-dimensional) linear transformation can be written as a matrix (in fact
	this is why matrix multiplication was invented).

	\question
	Define $\mathcal P$ to be projection onto $\vec u=\mat{2\\3}$.
	\begin{parts}
		\item Write down a matrix for $\mathcal P$.
		%\item What is the null space of $\mathcal P$?
		%\item What is the rank of the matrix corresponding to $\mathcal P$?
		%\item Is $\mathcal P$ invertible?
	\end{parts}

	Matrix multiplication was designed to exactly model composition of linear transformations.
	\begin{parts}[resume]
		\item Write down a matrix for $\mathcal P$ and for $\mathcal R$, the counter-clockwise rotation
			by $90^\circ$.
		\item Write down matrices for $\mathcal P\circ\mathcal R$ and $\mathcal R\circ \mathcal P$.
	\end{parts}

\newpage
	\begin{definition}[Range]
		The \emph{range} (or \emph{image}) of a linear transformation $T:V\to W$ is the set of vectors that
		$T$ can output.  That is,
		\[
			\Range(T)=\{\vec y\in W:\vec y=T\vec x\text{ for some }\vec x\in V\}.
		\]
	\end{definition}
	\begin{definition}[Null Space]
		The \emph{null space} (or \emph{kernel}) of a linear transformation $T:V\to W$ is the
		set of vectors that get mapped to zero under $T$.  That is,
		\[
			\Null(T)=\{\vec x\in V:T\vec x=\vec 0\}.
		\]
	\end{definition}

	\question
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the vector $\vec u=\mat{2\\3}$ (like before).
	\begin{parts}
		\item What is the range of $\mathcal P$?
		\item What is the null space of $\mathcal P$?
	\end{parts}


	\begin{definition}[Fundamental Subspaces]
		Associated with any matrix $M$ are three fundamental subspaces: the \emph{row space}
		of $M$ is the span of the rows of $M$; the \emph{column space} of $M$ is the span
		of the columns of $M$; and the \emph{null space} of $M$ is the set of solutions
		to $M\vec x=\vec 0$. 
		
	\end{definition}

	\question
	Consider $A=\mat{1&0&0\\0&1&0}$.
	\begin{parts}
		\item Describe the row space of $A$.
		\item Describe the column space of $A$.
		\item Is the row space of $A$ the same as the column space of $A$?
		\item Do the row space or column space of $A$ relate to the range
			of the linear transformation defined by $\mathcal A(\vec x) = A\vec x$?
		\item Describe the set of all vectors perpendicular to the rows of $A$.
		\item Describe the null space of $A$.
	\end{parts}

	\question
	\[
		B=\mat{1&2&3\\1&1&1}\qquad C=\rref(B)=\mat{1&0&-1\\0&1&2}
	\]
	\begin{parts}
		\item How does the row space of $B$ relate to the row space of $C$?
		\item How does the null space of $B$ relate to the null space of $C$?
		\item Compute the null space of $B$.
	\end{parts}

	\question
	\[
		P=\mat{0&0\\1&2}\qquad Q=\rref(P)=\mat{1&2\\0&0}
	\]
	\begin{parts}
		\item How does the column space of $P$ relate to the column space of $Q$?
		\item Describe the column space of $P$ and the column space of $Q$.
	\end{parts}


	\newpage
	\begin{definition}[One-to-One \& Onto]
		A linear transformation $T:\R^n\to\R^m$ is called \emph{one-to-one} if $T(\vec a)=T(\vec b)$
		only when $\vec a=\vec b$. The transformation $T$ is called \emph{onto} if its range is all
		of $\R^m$.
	\end{definition}

	\question
	Let $T:\R^3\to\R^3$ and $S:\R^3\to\R^3$ be matrix transformations
	and suppose 
	\[
		\text{null}(T)=\Span\{\xh\}\qquad \text{and}\qquad\text{null}(S)=\{\vec 0\}.
	\]
	\begin{parts}
		\item Could $T$ be one-to-one? Explain.
		\item Could $S$ be one-to-one? Explain.
		\item Could $S$ be onto? Explain.
		\item Could $T$ be onto? Explain.
	\end{parts}



\section*{Matrix Inverses}

	\question
	\begin{parts}
		\item Apply the row operation $R_3\to R_3+2R_1$ to the $3\times 3$ identity
		matrix and call the result $E_1$.
		\item Apply the row operation $R_3\to R_3-2R_1$ to the $3\times 3$ identity
		matrix and call the result $E_2$.
	\end{parts}

	\begin{definition}
	An \emph{elementary matrix} is the identity matrix with a single row operation applied.
	\end{definition}

	\[
		A=\mat{1&2&3\\4&5&6\\7&8&9}
	\]
	\begin{parts}[resume]
		\item Compute $E_1A$ and $E_2A$.  How do the resulting matrices relate to row
		operations?
		\item Without computing, what should the result of applying the row
		operation $R_3\to R_3-2R_1$ to $E_1$ be?  Compute and verify.
		\item Without computing, what should $E_1E_2$ be?  What about $E_2E_1$?
		Now compute and verify.
	\end{parts}

	\begin{definition}
		The \emph{inverse} of an $n\times n$ matrix $A$ is an $n\times n$
		matrix $B$ such that $AB=I_{n\times n}=BA$.
		In this case, $B$ is called the inverse of $A$ and is notated as $A^{-1}$.
	\end{definition}

	\question
	Consider the matrices 
	\[
		A=\mat{1&2&0\\0&1&0\\-3&-6&1}\qquad
		B=\mat{1&0&0\\0&1&0}\qquad
		C=\mat{1&0\\0&1\\0&0}
	\]
	\[
		D=\mat{1&-2&0\\0&1&0\\3&0&1}\qquad
		E=\mat{1&0&0\\0&2&0\\0&1&1}\qquad
		F=\mat{1&0&0\\0&1&0\\0&0&1}
	\]
	\begin{parts}
		\item Which pairs of matrices above are inverses of each other?
	\end{parts}

	\newpage
	\question
	\[
		B=\mat{1 &4\\0 &2}
	\]
	\begin{parts}
		\item Use two row operations to reduce $B$ to $I_{2\times 2}$
		and write an elementary matrix $E_1$ corresponding to the first operation
		and $E_2$ corresponding to the second.
		\item What is $E_2E_1B$?
		\item Find $B^{-1}$.
		\item Can you outline a procedure for finding the inverse of a matrix
		using elementary matrices?
	\end{parts}

	\question
	\[
		A=\mat{1&2&-1\\2&2&4\\1&3&-3}\qquad
		\vec b=\mat{1\\2\\3}\qquad
		C=[A|\vec b]\qquad
		A^{-1}=\mat{9&-3/2&-5\\-5&1&3\\-2&1/2&1}
	\]
	\begin{parts}
		\item What is $A^{-1}A$?
		\item What is $\rref(A)$?
		\item What is $\rref(C)$? (Hint, there is no need to actually do row reduction!)
		\item Solve the system $A\vec x=\vec b$.
	\end{parts}

	\question
	\begin{parts}
		\item For two square matrices $X,Y$, should $(XY)^{-1}=X^{-1}Y^{-1}$?
		\item If $M$ is a matrix corresponding to a non-invertible linear transformation $T$,
			could $M$ be invertible?
	\end{parts}



\section*{Subspaces and Bases}
	\vspace{-1em}
	\begin{definition}[Subspace]
		A \emph{subspace} $V\subseteq \R^n$ is a non-empty subset such that
		\begin{enumerate}
			\item[(i)] $\vec u,\vec v\in V$ implies $\vec u+\vec v\in V$.
			\item[(ii)] $\vec u\in V$ implies $k\vec u\in V$ for all scalars $k$.
		\end{enumerate}
	\end{definition}

	Subspaces give a mathematically precise definition of a ``flat space through the origin.''

	\question
	For each set, draw it and explain whether or not it is a subspace of $\R^2$.
	\begin{parts}
		\item $A=\{\vec x\in\R^2:\vec x=\mat{a\\0}\text{ for some }a\in\Z\}$.
		\item $B=\{\vec x\in\R^2:\vec x\neq \mat{0\\0}\}$.
		\item $C=\{\vec x\in\R^2:\vec x=\mat{0\\t}\text{ for some }t\in\R\}$.
		\item $D=\{\vec x\in\R^2:\vec x=\mat{0\\t}+\mat{1\\1}\text{ for some }t\in\R\}$.
		\item $E=\{\vec x\in\R^2:\vec x=\mat{0\\t}\text{ or }\vec x=\mat{t\\0}\text{ for some }t\in\R\}$.
		\item $F=\{\vec x\in\R^2:\vec x=t\mat{3\\1}\text{ for some }t\in\R\}$.
		\item $G=\span\left\{\mat{1\\1}\right\}$.
		\item $H=\span\{\vec u,\vec v\}$ for some unknown vectors $\vec u,\vec v\in\R^2$.
	\end{parts}
	
	\question
	Let $T:\R^n\to\R^m$ be an arbitrary linear transformation.
	\begin{parts}
		\item Show that the null space of $T$ is a subspace.
		\item Show that the range of $T$ is a subspace.
	\end{parts}



	\begin{definition}[Basis]
		A \emph{basis} for a subspace $V$ is a linearly independent set of vectors, $\mathcal B$,
		so that $\Span\mathcal B=V$.
	\end{definition}

	\question
	Let $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, $\vec w=\mat{1\\1\\0}$, and $V=\span\{\vec u,\vec v,\vec w\}$.
	\begin{parts}
		\item Describe $V$.
		\item Is $\{\vec u,\vec v,\vec w\}$ a basis for $V$?  Why or why not?
		\item Give a basis for $V$.
		\item Give another basis for $V$.
		\item Is $\Span\{\vec u,\vec v\}$ a basis for $V$?  Why or why not?
	\end{parts}

	\begin{definition}[Dimension]
		The \emph{dimension} of a subspace $V$ is the number of elements in a basis for $V$.
	\end{definition}

	\begin{parts}[resume]
		\item What is the dimension of $V$?
	\end{parts}


	\question
	Let $\vec a=\mat{1\\2\\3}$, $\vec b=\mat{4\\5\\6}$, $\vec c=\mat{7\\8\\8}$ and 
	let $P=\span\{\vec a,\vec b\}$ and $Q=\span\{\vec b,\vec c\}$.
	\begin{parts}
		\item Give a basis for and the dimension of $P$.
		\item Give a basis for and the dimension of $Q$.
		\item Is $P\cap Q$ a subspace? If so, give a basis for it and its dimension.
		\item Is $P\cup Q$ a subspace? If so, give a basis for it and its dimension.
	\end{parts}


\section*{Rank}
	\begin{definition}[Rank]
		The \emph{rank} of the matrix $A$ is the number of leading ones in the 
		reduced row echelon form of $A$.
	\end{definition}

	\question
	\begin{parts}
		\item Determine the rank of
		\begin{enumerate*}
			\item $\mat{1&1\\2&2}$
			\item $\mat{1&2\\3&4}$
			\item $\mat{1&1&0\\0&0&1}$
			\item $\mat{3\\3\\2}$
			\item $\mat{1&0&1\\0&1&0\\0&0&1}$.
		\end{enumerate*}
	\end{parts}
	
	\newpage
	\question
	Consider the homogeneous system 
		\begin{equation}\label{eq4b}
			\begin{array}{llll}
				x&+2y&+z &= 0\\
				x&+2y&+3z &= 0\\
				-x&-2y&+z &= 0
			\end{array}
		\end{equation}
	and the non-augmented matrix of coefficients $A=\mat{1&2&1\\1&2&3\\-1&-2&1}$.
	\begin{parts}
		\item What is $\Rank(A)$?
		\item Give the general solution to \eqref{eq4b}.
		\item Are the column vectors of $A$ linearly independent?
		\item Give a non-homogeneous system with the same coefficients as \eqref{eq4b} that has
			\begin{enumerate}
				\item infinitely many solutions
				\item no solutions.
			\end{enumerate}
	\end{parts}

	\question
	\begin{parts}
		\item The rank of a $3\times 4$ matrix $A$ is $3$.  Are the column vectors of $A$ linearly independent?
		\item The rank of a $4\times 3$ matrix $B$ is $3$.  Are the column vectors of $B$ linearly independent?
	\end{parts}




	\begin{theorem}[Rank-nullity Theorem]
	The \emph{nullity} of a matrix is the dimension of the null space.

	The rank-nullity theorem states
	\[
		\rank(A)+\nnul(A) = \#\text{ of columns in }A.
	\]
	\end{theorem}

	\question
	The vectors $\vec u,\vec v\in\R^9$ are linearly independent and $\vec w=2\vec u-\vec v$.
	Define $A=[\vec u|\vec v|\vec w]$.
	\begin{parts}
		\item What is the rank and nullity of $A^T$?
		\item What is the rank and nullity of $A$?
	\end{parts}

\newpage
\subsection*{Orthogonality}
	\begin{definition}[Orthogonal \& Orthonormal]
		A set of vectors is \emph{orthogonal} if every pair of vectors
		in the set is orthogonal.  A set of vectors is \emph{orthonormal}
		if it is both an orthogonal set and every vector is a unit vector.
	\end{definition}
	\begin{definition}[Orthogonal Projection]
		If $V$ is a subspace of $\R^n$, the \emph{projection}
		(sometimes called the orthogonal projection) of $\vec x$ onto $V$
		is the closest point in $V$ to $\vec x$. We notate the projection
		of $\vec x$ onto $V$ as $\proj_V\vec x$.
	\end{definition}

	Projections are normally hard to compute and a priori might require some sort
	of calculus-style optimization to find.  However, from geometry we know that 
	if we travel from $\proj_V \vec x$ to $\vec x$, we should always trace out a path
	perpendicular to $V$.  Otherwise, we could find a point in $V$ that was slightly closer
	to $\vec x$, violating the definition of $\proj_V \vec x$.  Thus, orthogonality
	will be our savior.

	\question
	Let $\mathcal S=\{\vec e_1,\vec e_2,\vec e_3\}$ be the standard basis.
	\begin{parts}
		\item If $\vec x=1\vec e_1+2\vec e_2+3\vec e_3$, find the projection of $\vec x$
			onto the $xy$-plane.
	\end{parts}
	Suppose $\mathcal B=\{\vec b_1,\vec b_2,\vec b_3\}$ is an orthonormal basis for $\R^3$.
	\begin{parts}[resume]
		\item If $\vec y=3\vec b_1-2\vec b_2+2\vec b_3$, find the projection of $\vec y$
			onto $\span\{\vec b_1,\vec b_3\}$.
	\end{parts}
	Suppose $\mathcal C=\{\vec c_1,\vec c_2,\vec c_3\}$ is a basis for $\R^3$ with
	\[
		\|\vec c_1\| = 
		\|\vec c_2\| = 
		\|\vec c_3\| = 1\qquad \vec c_1\cdot \vec c_2=0\qquad \vec c_1\cdot \vec c_3=0
		\qquad \vec c_2\cdot \vec c_3=\sqrt{2}/2.
	\]
	\vspace{-.35in}
	\begin{parts}[resume]
		\item If $\vec z=5\vec c_1+2\vec c_2-\vec c_3$, find the projection of $\vec z$
			onto $\span\left\{\vec c_1,\vec c_2\right\}$.
	\end{parts}

	\question
	Let's put this all together.  
	$\mathcal B=\left\{\mat{2\\1\\1},\mat{1\\-1\\-1},\mat{0\\1\\-1}\right\}$ is an
	orthogonal basis for $\R^3$.  Let $\mathcal P$ be the plane defined
	by
	\[
		0x+y-z=0.
	\]
	\begin{parts}
		\item Write $\mathcal P$ in vector form (Hint: think about the vectors
			listed in the $\mathcal B$ basis).
		\item Find an orthonormal basis $\mathcal C=\{\vec c_1,\vec c_2,\vec c_3\}$
			for $\R^3$ so $\mathcal P=\span\{\vec c_1,\vec c_2\}$.
		\item Let $\vec x=\mat{1\\2\\3}$.  Find $\proj_{\mathcal P}\vec x$.
	\end{parts}

\newpage
\subsection*{Gram-Schmidt Orthogonalization}
	We've seen how useful orthonormal bases are.  The incredible thing is that we can 
	turn any basis into an orthonormal basis through a process called
	Gram-Schmidt orthogonalization.

	\question
	Let $\vec a=\mat{1\\2}$ and $\vec b=\mat{3\\1}$.
	\begin{parts}
		\item Draw $\vec a$ and $\vec b$ and find $\vec w=\proj_{\vec b}\vec a$.
		\item Add $\vec c=\vec a-\vec w$ to your drawing.  What is the angle between
			$\vec c$ and $\vec b$.
		\item Can you write $\vec a$ as the sum of two vectors, one in 
			the direction of $\vec b$ and one orthogonal to $\vec b$?
			If so, do it.
	\end{parts}

	\question
	Let $\vec a=\mat{1\\2\\6}$ and $\vec b=\mat{1\\1\\-1}$.
	\begin{parts}
		\item Write $\vec a=\vec u+\vec v$ where $\vec u$ is parallel to
			$\vec b$ and $\vec v$ is orthogonal to $\vec b$.
		\item Find an orthonormal basis for $\span\{\vec a,\vec b\}$.
	\end{parts}

	With two vectors, making an orthonormal set without changing the span
	is quite easy.  With more vectors, it is only slightly harder.


	\begin{definition}[Gram-Schmidt Process]
		The \emph{Gram-Schmidt} orthogonalization procedure
		takes in a set of vectors and outputs a set of orthonormal vectors
		with the same span.  The idea is to iteratively produce a set of
		vectors where each new vector you produce is orthogonal to the previous vectors.

		The algorithm is as follows: Let $\{ v_1,\ldots, v_n\}$ be a set of 
		vectors.  Produce a set $\{ v_2',\ldots, v_n'\}$ that is orthogonal
		to $ v_1$ by subtracting off the respective projections
		of $ v_2,\ldots, v_n$
		onto $ v_1$.  Next, produce a set $\{ v_3'',\ldots, v_n''\}$
		orthogonal to both $ v_1$ and $ v_2'$ by subtracting off the
		respective projections
		onto $ v_2'$.  Continue this process until you have a set
		$V=\{ v_1, v_2', v_3'', v_4''',\ldots\}$ that is orthogonal.
		Finally, normalize $V$ so all vectors have unit length.
	\end{definition}

	\question
	Let $\vec x_1=\mat{1\\-1\\-1\\1}$, $\vec x_2=\mat{2\\1\\0\\1}$, and 
	$\vec x_3=\mat{2\\2\\1\\2}$.
	\begin{parts}
		\item Use the Gram-Schmidt procedure to find an orthonormal basis for 
			$\span\{\vec x_1,\vec x_2,\vec x_3\}$.
		\item Find an orthonormal basis $\mathcal V=\{\vec v_1,\vec v_2,\vec v_3,\vec v_4\}$
			for $\R^4$ so that $\span\{\vec v_1,\vec v_2,\vec v_3\}=
			\span\{\vec x_1,\vec x_2,\vec x_3\}$.
	\end{parts}
	Let $R=\mat{1&-1&-1&1\\2&1&0&1\\2&2&1&2}$.
	\begin{parts}[resume]
		\item Find an orthonormal basis for the row space of $R$.
		\item Find the null space of $R$ (Hint, you've already done the work, so
			there is no need to row reduce).
	\end{parts}

	\newpage
	\question
	Let
	\[
		\vec y_1=\mat{1\\1\\2}\qquad 
		\vec y_2=\mat{-1\\-1\\2}\qquad
		\vec y_3=\mat{1\\1\\6} \qquad\text{and}\qquad \mathcal W=\span\{\vec y_1,\vec y_2,\vec y_3\}.
	\]
	\begin{parts}
		\item Find an orthonormal basis $\mathcal B$ for $\mathcal W$.
	\end{parts}

	\begin{definition}[Orthogonal Complement]
		The \emph{orthogonal complement} of a subspace $V$ is written
		$V^\perp$ and defined as
		\[
			V^\perp=\{\vec x:\vec x\text{ is orthogonal to all vectors in }V\}.
		\]
		\vspace{-.2in}
	\end{definition}

	\begin{parts}[resume]
		\item Find the orthogonal complement of $\mathcal W$.
		\item Write $\vec v=\mat{1\\0\\1}$ in the form $\vec v=\vec r+\vec n$ where 
			$\vec r\in\mathcal W$ and $\vec n\in{\mathcal W}^\perp$.
	\end{parts}


	\begin{theorem}[Orthogonal Decomposition]
		If $\mathcal V\subseteq \R^n$ is a subspace than any vector $\vec x\in\R^n$ can be written uniquely as
		$
			\vec x=\vec v+\vec w
		$
		where $\vec v\in\mathcal V$ and $\vec w\in{\mathcal V}^\perp$.
	\end{theorem}

	\question
	Let $\vec x=\mat{1\\2\\3}$. For each of the following $\mathcal V$, decompose $\vec x$ into the sum of a vector in
	$\mathcal V$ and one in ${\mathcal V}^\perp$.
	\begin{parts}
		\item $\mathcal V$ is the $x$-axis.
		\item $\mathcal V=\{\vec 0\}$.
		\item $\mathcal V$ is the plane $x+y+z=0$.
		\item $\mathcal V=\R^3$.
	\end{parts}


	\begin{definition}[Direct Sum]
		Suppose $\mathcal V,\mathcal W\subseteq \R^n$ are two subspaces and every vector $\vec x\in\R^n$
		can be uniquely expressed as $\vec x=\vec v+\vec w$ for $\vec v\in\mathcal V$ and $\vec w\in\mathcal W$.
		Then we say $\R^n$ is the \emph{direct sum} of $\mathcal V$ and $\mathcal W$ and write
		\[
			\R^n=\mathcal V\oplus \mathcal W.
		\]
	\end{definition}

	\begin{theorem}[Fundamental Theorem of Linear Algebra]
		Let $A$ be an $n\times m$ matrix. Then 
		\[
			\text{row}(A)=\Null(A)^\perp\qquad\text{and}\qquad \R^m = \text{row}(A)\oplus \Null(A)
		\]
		and
		\[
			\text{col}(A)=\Null(A^T)^\perp\qquad\text{and}\qquad \R^n = \text{col}(A)\oplus \Null(A^T).
		\]
	\end{theorem}

	\question
	Let $A=\mat{1&1\\1&1}$ and let $\vec b=\mat{4\\4}$.
	\begin{parts}
		\item Verify that $\vec u=\mat{4\\0}$, $\vec v=\mat{5\\-1}$, and $\vec w=\mat{-6\\10}$ are all solutions
			to $A\vec x=\vec b$.
		\item Decompose $\vec u$, $\vec v$, and $\vec w$ into the sum of vectors in $\text{row}(A)$ and $\Null(A)$. What do you notice?

		\item Suppose $\vec x\in\text{row}(A)$ and $A\vec x=\vec b$. What is $\vec x$? Is it unique?

		\item Consider the transformation $T:\text{row}(A)\to\R^2$ where $T(\vec y)=A\vec y$. Is $T$ invertible? Is $A$ invertible? Explain.

	\end{parts}

	\newpage
\section*{Determinants}
	\begin{definition}[Unit $n$-cube]
		The unit $n$-cube is the $n$-dimensional cube with side length 1 and lower-left
		corner located at the origin.  That is 
		\[
			C_n = \left\{\vec x\in\R^n:\vec x=\sum_{i=1}^n \alpha_i\vec e_i\text{ for some }\alpha_1,\ldots,\alpha_n\in[0,1]\right\}=[0,1]^n.
		\]
	\end{definition}
	The volume of the unit $n$-cube is always 1.

	\question
	The picture shows what the linear transformation $T$ does to the unit square (i.e., the unit $2$-cube).

	\begin{center}
	\includegraphics[width=2.5in]{images/transform1b.pdf}
	\includegraphics[width=2.5in]{images/transform2b.pdf}
	\end{center}

	\vspace{-6em}
	\begin{parts}
		\item What is $T\mat{1\\0}$, $T\mat{0\\1}$, $T\mat{1\\1}$?
		\item Write down a matrix for $T$.
		\item What is the volume of the image of the unit square (i.e., the volume of $T(C_2)$)?  You may need
			to use trigonometry.
	\end{parts}
	
	\begin{definition}[Determinant]
	The \emph{determinant} of a linear transformation $X:\R^n\to \R^n$ is the 
	oriented volume of the image of the unit $n$-cube.  The determinant
	of a square matrix is the oriented volume of the parallelepiped 
	($n$-dimensional parallelogram) given by the column vectors or the row
	vectors.
	\end{definition}

	\question
	We know the following about the transformation $A$:
	\[
		A\mat{1\\0}=\mat{2\\0}\qquad\text{and}\qquad A\mat{0\\1}=\mat{1\\1}.
	\]
	\begin{parts}
	\item Draw $C_2$ and $A(C_2)$, the image of the unit square
			under $A$.
		\item Compute the area of $A (C_2)$.
		\item Compute $\det(A)$.
	\end{parts}

	\question
	Suppose $R$ is a rotation counterclockwise by $30^\circ$.
	\begin{parts}
	\item Draw $C_2$ and $R(C_2)$.
	\item Compute the area of $R(C_2)$.
		\item Compute $\det(R)$.
	\end{parts}
	
	\question
	We know the following about the transformation $F$:
	\[
		F\mat{1\\0}=\mat{0\\1}\qquad\text{and}\qquad F\mat{0\\1}=\mat{1\\0}.
	\]
	\begin{parts}
		\item What is $\det(F)$?
	\end{parts}

	\question
	\begin{itemize}
		\item $E_f$ is $I_{3\times 3}$ with the first two rows swapped.
		\item $E_m$ is $I_{3\times 3}$ with the third row multiplied by 6.
		\item $E_a$ is $I_{3\times 3}$ with $R_1\to R_1+2R_2$ applied.
	\end{itemize}

	\begin{parts}
		\item What is $\det(E_f)$?
		\item What is $\det(E_m)$?
		\item What is $\det(E_a)$?
		\item What is $\det(E_fE_m)$?
		\item What is $\det(4I_{3\times 3})$?
		\item What is $\det(W)$ where $W=E_fE_aE_fE_mE_m$?
	\end{parts}

	\question
	$U=\mat{1&2&1&2\\0&3&-2&4\\0&0&-1&0\\0&0&0&4}$
	\begin{parts}
		\item What is $\det(U)$?
		\item $V$ is a square matrix and rref$(V)$ has a row of zeros.
		 What is $\det(V)$?
		\item $P$ is projection onto the vector $\mat{-1\\-1}$. What is $\det(P)$?
	\end{parts}

	\question
	Suppose you know $\det(X)=4$.
	\begin{parts}
		\item What is $\det(X^{-1})$?
		\item Derive a relationship between $\det(Y)$
			and $\det(Y^{-1})$ for an arbitrary matrix $Y$.
		\item Suppose $Y$ is not invertible.  What is $\det(Y)$?
	\end{parts}

	\newpage
\section*{Eigenvectors}

	\vspace{-.6cm}
	\begin{definition}[Eigenvector]
	For a linear transformation $X$, an \emph{eigenvector} for $X$ is a non-zero vector that doesn't
	change directions when $X$ is applied.  That is, $\vec v\neq \vec 0$ is an eigenvector for $X$ if
	\[
		X\vec v=\lambda \vec v
	\]
	for some scalar $\lambda$.  We call $\lambda$ the \emph{eigenvalue} 
	of $X$ corresponding
	to the eigenvector $\vec v$.
	\end{definition}
	\vspace{-.2cm}

	\question
	The picture shows what the linear transformation $T$ does to the unit square (i.e., the unit $2$-cube).
	
	\vspace{-1cm}
	\begin{center}
	\includegraphics[width=2in]{images/transform1b.pdf}
	\includegraphics[width=2in]{images/transform2b.pdf}
	\end{center}
	\vspace{-2.5cm}

	\begin{parts}
		\item Give an eigenvector for $T$.  What is the eigenvalue?
		\item Can you give another?
	\end{parts}


	\question
	For some matrix $A$,
	\vspace{-.2cm}
	\[
		A\mat{3\\3\\1}=\mat{2\\2\\2/3}\qquad\text{ and }\qquad B=A-\tfrac{2}{3}I.
	\]
	\vspace{-.4cm}
	\begin{parts}
		\item Give an eigenvector and a corresponding eigenvalue for $A$.
		\item What is $B\mat{3\\3\\1}$?
		\item What is the dimension of $\text{null}(B)$?
		\item What is $\det(B)$?
	\end{parts}

	\vspace{-.2cm}
	\question
	Let $C=\mat{-1&2\\1&0}$ and $E_\lambda = C-\lambda I$.
	\begin{parts}
		\item For what values of $\lambda$ does $E_\lambda$ have a non-trivial
			null space?
		\item What are the eigenvalues of $C$?
		\item Find the eigenvectors of $C$.
	\end{parts}
	
	\newpage
	\begin{definition}[Characteristic Polynomial]
	For a matrix $A$, the \emph{characteristic polynomial} of $A$ is
	\[
		\chr(A)=\det(A-\lambda I).
	\]
	\end{definition}
	\vspace{-.4cm}
	
	\question
	Let $D=\mat{1&2\\3&0}$.
	\begin{parts}
		\item Compute $\chr(D)$.
		\item Find the eigenvalues of $D$.
	\end{parts}

	\vspace{-.3cm}
	\question
	\vspace{-.2cm}
	Suppose $\chr(E)=\lambda(\lambda -2)(\lambda +3)$ for some unknown $3\times 3$
	matrix $E$.
	\begin{parts}
		\item What are the eigenvalues of $E$?
		\item Is $E$ invertible?
		\item What is $\nnul(E)$, $\nnul(E-3I)$, $\nnul(E+3I)$?
	\end{parts}

	\question
	Consider
	\[
		A=\mat{1&0&1\\0&1&1\\1&1&0}\qquad
		\vec v_1=\mat{1\\1\\1}\qquad
		\vec v_2=\mat{1\\1\\-2}\qquad
		\vec v_3=\mat{-1\\1\\0}
	\]
	and notice that $\vec v_1,\vec v_2,\vec v_3$ are eigenvectors for $A$.
	\begin{parts}
		\item Find the eigenvalues of $A$.
		\item Find the characteristic polynomial of $A$.
		\item Compute $A\vec w$ where $w=2\vec v_1-\vec v_2$.
		\item Compute $A\vec u$ where $\vec u=a\vec v_1+b\vec v_2+c\vec v_3$ for
			unknown scalar coefficients $a,b,c$.
	\end{parts}
	Notice that $\mathcal V=\{\vec v_1,\vec v_2,\vec v_3\}$ is a basis for $\R^3$.
	\begin{parts}[resume]
	\item Imagine for a moment that $\vec v_1=\xh$, $\vec v_2=\yh$ and $\vec v_3=\zh$.
		In this fantasy world, $\vec v_1$, $\vec v_2$, and $\vec v_3$
		are still eigenvectors with the eigenvalues you've already found.
		What would the matrix $A$ be in this fantasy?
	\end{parts}
	
	\question
	Let $A$, $\vec v_1$, $\vec v_2$, and $\vec v_3$ be as in the previous problem, let
	$P=[\vec v_1|\vec v_2|\vec v_3]$ be the matrix with columns $\vec v_1$, $\vec v_2$, and $\vec v_3$,
	and
	let $\vec b\in\R^3$ be a fixed vector.
	\begin{parts}
		\item Describe what a solution to $P\vec x=\vec b$ means in terms of $\vec v_1$, $\vec v_2$,
			and $\vec v_3$.
		\item Describe how to interpret the output of the linear transformation
			$P^{-1}$ in terms of $\vec v_1$, $\vec v_2$, and $\vec v_3$.
		\item Describe how you can use $P$ and $P^{-1}$ to easily compute
			$A\vec y$ for any $\vec y\in \R^3$.
		\item Can you find a matrix $D$ so that
			\[
				PDP^{-1}=A?
			\]
		\item Suppose $P^{-1}\vec b=\mat{1\\3\\4}$.  Compute $A^{100}\vec b$.
	\end{parts}


	\question
	For an $n\times n$ matrix $T$, suppose its eigenvectors $\{\vec v_1,\ldots \vec v_n\}$
	form a basis for $\R^n$.  Let $\lambda_1,\ldots,\lambda_n$ be the corresponding
	eigenvalues.
	

	\begin{parts}
	\item Is $T$ diagonalizable (i.e., similar to a diagonal matrix)?  If so, explain how to obtain its diagonalized form.
		\item What if one of the eigenvalues of $T$ is zero?  Is $T$ diagonalizable?
		\item What if the eigenvectors of $T$ did not form a basis for $\R^n$.
			Would $T$ be diagonalizable?
	\end{parts}

	\begin{definition}[Eigenspace]
	Let $A$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_m\}$.  The
	\emph{eigenspace} of $A$ corresponding to the eigenvalue $\lambda_i$ is the
	null space of $A-\lambda_i I$.  That is, it is the space spanned by all eigenvectors
	that have the eigenvalue $\lambda_i$.

	The \emph{geometric multiplicity} of an eigenvalue $\lambda_i$ is the dimension
	of the eigenspace corresponding to $\lambda_i$.  The \emph{algebraic multiplicity}
	of $\lambda_i$ is the number of times $\lambda_i$ occurs as a root of the
	characteristic polynomial of $A$ (i.e., the number of times $x-\lambda_i$
	occurs as a factor).
	\end{definition}

	\question
	Define $F=\mat{1&1\\0&1}$.
	\begin{parts}
		\item Is $F$ diagonalizable?  Why or why not?
		\item What is the geometric and algebraic multiplicity of each eigenvalue
			of $F$?
		\item Suppose $A$ is a matrix where the geometric multiplicity of one of its eigenvalues
			is smaller than the algebraic multiplicity of the same eigenvalue.  Is
			$A$ diagonalizable?  What if all the geometric and algebraic multiplicities
			match?
	\end{parts}


\subsection*{Symmetric Matrices}
	When you're new to Linear Algebra, learning lots of new concepts and algorithms,
	it's sometimes hard to grasp the significance of certain properties of a matrix.

	Symmetric matrices are easy to forget at first, but they have many profound 
	properties (not to mention they are one of the key concepts of Quantum Mechanics).

	\question
	Let $A$ be a symmetric matrix and let $\vec v$ be an eigenvector with eigenvalue
	3 and $\vec w$ be an eigenvector with eigenvalue 4.  Note, for this problem,
	we are thinking of $\vec v$ and $\vec w$ as column vectors.
	\begin{parts}
		\item Write $A\vec v$, $\vec v^TA^T$, $\vec v^TA$, $A\vec w$, $\vec w^TA^T$, 
		and $\vec w^TA$ in terms of $\vec v$, $\vec w$ and scalars.
		\item How do $\vec v^T\vec w$ and $\vec w^T\vec v$ relate?
		\item What should $\vec v^TA\vec w$ be in terms of $\vec v^T$ and
			$\vec w$? (Note, you could compute $(\vec v^TA)\vec w$
			or $\vec v^T(A\vec w)$.  Better do both to be safe).
		\item What can you conclude about $\vec v^T\vec w$?  How about
			$\vec v\cdot \vec w$?
	\end{parts}

	We've just deduced that all eigenspaces of a symmetric matrix are orthogonal! On
	top of that, symmetric matrices always have a basis of eigenvectors.  That means
	that not only can you always diagonalize a symmetric matrix, but you can 
	\emph{orthogonally} diagonalize a symmetric matrix. (i.e. if $A$ is symmetric,
	then $A=QDQ^T$ where $Q$ is orthogonal and $D$ is diagonal).  This is like the 
	best of all worlds in one!
























	\newpage
\section*{Additional Problems}
	\question
	\[
		A=\mat{1&2\\3&1\\0&-1}
		\qquad
		B=\mat{-1&-1\\0&1\\1&-2}
		\qquad
		C=\mat{1&2&0\\-1&-1&-1}
	\]
	\begin{parts}
		\item Write the shape of the matrices $A,B,C$ (i.e., for each one,
		write the dimensions in $m\times n$ form).
		\item List \emph{all} products between the matrices $A,B,C$ that are
		defined. (Your list will be some subset of $AB,AC,BA,CA,BC,CB$.)
		\item Compute $AC$ and $CA$.
	\end{parts}

	\question
	\begin{parts}
		\item If the matrices $X$ and $Y$ are both square $n\times n$ matrices,
		does $XY=YX$?  Explain.
		\item If the matrices $X$ and $Y$ are both square $n\times n$ matrices,
		does $X+Y=Y+X$?  Explain.
	\end{parts}

	\question
	Consider the system represented by
	\[
		\mat{1&-3&0\\0&0&1\\0&0&0}\mat{x\\y\\z}=\vec b.
	\]
	\begin{parts}
		\item If $\vec b=\mat{1\\2\\3}$, is the set of solutions to this system a 
		point, line, plane, or other?
		\item If $\vec b=\mat{1\\1\\0}$, is the set of solutions to this system a 
		point, line, plane, or other?
	\end{parts}

	\question
	The entries of a matrix are specified by (row,column) pairs of integers.  If
	$a_{ij}$ is the $(i,j)$ entry of a matrix $A$, we may write $A=[a_{ij}]$.
	\begin{parts}
		\item Write the $2\times 2$ matrix $A$ with entries $a_{11} = 4$, $a_{12}=3$,
			$a_{21} = 7$ and $a_{22}=9$.
		\item Let $B=[b_{ij}]$ be the $3\times 3$ matrix where $b_{ij} = i+j$.  Write $B$.
		\item Let $C=[c_{ij}]$ be the $3\times 4$ matrix where $c_{ij} = 0$ if $i=j$ and
			$c_{ij}=1$ if $i\neq j$.
	\end{parts}

	\question
	\begin{definition}
		The \emph{transpose} of a matrix $A=[a_{ij}]$ is the matrix $A^{T}=[a_{ji}]$.
	\end{definition}
	Visually, the transpose of a matrix swaps rows and columns.

	\[
		A=\mat{1&1&2\\2&2&1}
	\]
	\begin{parts}
		\item What is the shape of $A$ and $A^T$?
		\item Write down $A^T$.
	\end{parts}

	$B$ and $D$ are $4\times 6$ matrices and $C$ is a $6\times 4$ matrix.

	\begin{parts}[resume]
		\item Does $(BC)^T=B^TC^T$? Explain.
		\item Does $(B+D)^T=B^T+D^T$? Explain.
		\item Compute $AA^T$ and $A^TA$ (where $A$ is the matrix defined earlier).
		What do you notice?
	\end{parts}

	\question
	\begin{definition}
		A matrix $X$ is called \emph{symmetric} if $X=X^T$.  
	\end{definition}
	Symmetric matrices have many useful properties,
	and have deep connections with orthogonality and eigenvectors (which we will get to later on).

	\begin{parts}
		\item Prove that if $W$ is a square matrix, then $V=W^TW+W+W^T$ is a symmetric
		matrix.
	\end{parts}

	\question
	\begin{definition}
		A \emph{zero matrix} is a matrix whose entries are all zeros.
		An \emph{identity matrix} is a square matrix whose diagonal
		entries are $1$ and non-diagonal entries are $0$.
	\end{definition}
	We write the $m\times n$ zero matrix as $0_{m\times n}$ or just $0$ if the shape
	is determined by context.  The $n\times n$ identity matrix is notated $I_{n\times n}$ or just
	$I$ if the shape is determined by context.

	Let $A=\mat{1&2&3\\4&5&6\\7&8&9}$.
	\begin{parts}
		\item Write down the $3\times 3$ identity matrix and the $3\times 3$ zero
		matrix.
		\item Compute $I_{3\times 3}A$, $AI_{3\times 3}$, $0_{3\times 3}A$,
		and $A0_{3\times 3}$.
		\item If we were to think of matrices as numbers, what numbers would the
		zero matrix and the identity matrix correspond to?
	\end{parts}

	\question
	\begin{parts}
		\item Solve the matrix equation
		\[
			I_{4\times 4}\mat{x\\y\\z\\w} = \mat{2\\3\\1\\-1}.
		\]
	\end{parts}



\section*{Change of Basis}
	\question
	Let $\vec b_1=\mat{1\\1}$, $\vec b_2=\mat{1\\-1}$, $\vec c=\mat{4\\0}$, and $\mathcal B=\{\vec b_1,\vec b_2\}$.
	\begin{parts}
		\item Is $\mathcal B$ a basis for $\R^2$?
		\item Find coefficients $\alpha_1$ and $\alpha_2$ so that $\vec c=\alpha_1\vec b_1+\alpha_2\vec b_2$.
	\end{parts}
	We call the vector $\mat{\alpha_1\\\alpha_2}$ the representation of $\vec c$ in the $\mathcal B$ basis and notate
	this by $[\vec c]_{\mathcal B}=\mat{\alpha_1\\\alpha_2}$.
	\begin{parts}[resume]
		\item Compute $[\vec e_1]_{\mathcal B}$ and $[\vec e_2]_{\mathcal B}$.
	\end{parts}
	Let $X=[\vec b_1|\vec b_2]$ be the matrix whose columns are $\vec b_1$ and $\vec b_2$.
	\begin{parts}[resume]
		\item Compute $X[\vec c]_{\mathcal B}$.  What do you notice?
	\end{parts}

	
	\question
	Let $\mathcal S=\{\vec e_1,\vec e_2,\ldots,\vec e_n\}$ be the standard basis for $\R^n$.
	Given a basis $\mathcal B=\{\vec b_1,\vec b_2,\ldots,\vec b_n\}$ for $\R^n$, the 
	matrix $X=[\vec b_1|\vec b_2|\cdots|\vec b_n]$ converts
	vectors from the $\mathcal B$ basis into the standard basis.  In other words,
	\[
		X[\vec v]_{\mathcal B} = [\vec v]_{\mathcal S}.
	\]
	\begin{parts}
		\item Should $X^{-1}$ exist? Explain.
		\item Consider the equation\[
				X^{-1}[\vec v]_{?} = [\vec v]_{?}.
			\]
			Can you fill in the ``?'' symbols so that the equation makes sense?
		\item What is $[\vec b_1]_{\mathcal B}$?  How about $[\vec b_2]_{\mathcal B}$?  Can
			you generalize to $[\vec b_i]_{\mathcal B}$?
	\end{parts}

	\question
	Let $\vec c_1=\mat{2\\1}$, $\vec c_2=\mat{5\\3}$, $\mathcal C=\{\vec c_1,\vec c_2\}$, and $A=\mat{2&5\\1&3}$.
	Note that $A^{-1}=\mat{3&-5\\-1&2}$ and that $A$ changes vectors from the $\mathcal C$ basis to the standard
	basis and $A^{-1}$ changes vectors from the standard basis to the $\mathcal C$ basis.
	\begin{parts}
		\item Compute $[\vec c_1]_{\mathcal C}$ and $[\vec c_2]_{\mathcal C}$.
	\end{parts}
	Let $T:\R^2\to\R^2$ be the linear transformation that stretches in the $\vec c_1$ direction by a factor of $2$
	and doesn't stretch in the $\vec c_2$ direction at all.
	\begin{parts}[resume]
		\item Compute $T\mat{2\\1}$ and $T\mat{5\\3}$.
		\item Compute $[T\vec c_1]_{\mathcal C}$ and $[T\vec c_2]_{\mathcal C}$.
		\item Compute the result of $T\mat{\alpha\\\beta}_{\mathcal C}$ and express the result in the
			$\mathcal C$ basis (i.e., as a vector of the form $\mat{?\\?}_{\mathcal C}$).
		\item Find a matrix for $T$ in the $\mathcal C$ basis.
		\item Find a matrix for $T$ in the standard basis.
	\end{parts}
	\begin{definition}[Similar Matrices]
		A matrix $A$ and a matrix $B$ are \emph{similar matrices}, denoted $A\sim B$, if
		$A$ and $B$ represent the same linear transformation but in possibly different bases.
		Equivalently, $A\sim B$ if there is an invertible matrix $X$ so that
		\[
			A=XBX^{-1}.
		\]
	\end{definition}




	After all this work with determinants, we see 
	that (like dot products) there is a geometric and an
	algebraic way of thinking about them, and they 
	\emph{determine} if a matrix is invertible.

	\question
	\begin{parts}
		\item The linear transformation $L:\R^3\to\R^3$ is a change of coordinates and $\det(L)=-4$.
			What is the volume form for this change of coordinates?
		\item Suppose $P:\R^2\to\R^2$ is the parameterization defined by $P\left(\mat{x\\y}\right)=\mat{1&2\\3&9}\mat{x\\y}+\mat{1\\1}$.
			Find the volume form for $P$.
		\item Suppose $p:\R^2\to\R^2$ is the parameterization defined by $p(r,\theta)=(r\cos\theta,r\sin\theta)$.  Find
			a linear approximation to $p$ at the point $(r_0,\theta_0)$.  Use determinants to compute the volume form for $p$
			at $(r_0,\theta_0)$.
	\end{parts}
	\begin{definition}[Jacobian]
		Let $p:\R^n\to\R^n$ be a parameterization.  Let $L_{\vec x_0}(\vec x) = J_{\vec x_0}\vec x+\vec q_{\vec x_0}$
		be the linear approximation to $p$ at the point $\vec x_0$.  The \emph{Jacobian} of $p$ at the point $\vec x_0$
		is defined to be
		\[
			\mathrm{Jacob}_{\vec x_0}(p) = \det(J_{\vec x_0}).
		\]
	\end{definition}



\section*{Orthogonality}

	\question
	\[
		\mathcal B=\{\vec b_1,\vec b_2\}\qquad\vec b_1=\mat{1/2\\\sqrt{3}/2}
		\qquad \vec b_2=\mat{-\sqrt{3}/2\\1/2}
	\]
	The matrix $A=[\vec b_1|\vec b_2]$ takes vectors in the $\mathcal B$ basis
	and rewrites them in the standard basis.
	\begin{parts}
		\item What does $A^{-1}$ do?
		\item Find a matrix $B$ that takes vectors in the standard basis
			and rewrites them in the $\mathcal B$ basis.
		\item Write $\vec x=\mat{1\\2}$ in the $\mathcal B$ basis.
		\item What is the relationship between $A$ and $B$?
	\end{parts}

	\begin{definition}[Orthogonal Matrix]
		An \emph{orthogonal matrix} is a square matrix whose columns are
		orthonormal (Yes, a better name would be orthonormal matrix, but that
		is not the term the rest of the world uses).
	\end{definition}

	\question
	Suppose $X=[\vec x_1|\vec x_2|\vec x_3|\vec x_4]$ is an orthogonal matrix.
	\begin{parts}
		\item What is the shape of $X$ (i.e., it is a what$\times$what matrix)?
		\item Compute $X^TX$.
		\item What is $X^{-1}$?
	\end{parts}

	\question
	\[
		Y=\mat{1&1&1&-1\\1&-1&-1&-1\\1&1&-1&1\\1&-1&1&1}
	\]
	\begin{parts}
		\item Is $Y$ an orthogonal matrix?
		\item Fix $Y$ so it is an orthogonal matrix.  Call the new matrix $X$.
		\item Compute $X^{-1}$.
		\item Compute $Y^{-1}$.
		\item Compute $|\det(X)|$ and $|\det(Y)|$ (the absolute value of
			the determinant of $X$ and $Y$).
	\end{parts}

	Matrix equations involving orthogonal matrices are easy to solve because the
	inverse of an orthogonal matrix is so easy to compute!
	
	\question
	Let $A=[\vec a_1|\vec a_2|\vec a_3|\vec a_4]$ be an orthogonal matrix.
	\begin{parts}
		\item Explain why 
			$\vec x=\mat{\vec a_1\cdot \vec b\\
				     \vec a_2\cdot \vec b\\
			     	     \vec a_3\cdot \vec b\\
			     	     \vec a_4\cdot \vec b}$ is a solution to $A\vec x=\vec b$.
		\item Find scalars $a,b,c,d$ so $\vec b=a\vec a_1+b\vec a_2+c\vec a_3+d\vec a_4$
			(your answers will have variables in them).
	\end{parts}

	Orthogonal matrices also allow us to compute projections quite easily.



\subsection*{$QR$ Decomposition}

	\begin{definition}[$QR$ Decomposition]
		For a matrix $A$, we can rewrite $A=QR$ where $Q$ is an
		orthogonal matrix and $R$ is an upper triangular matrix.  Writing
		$A$ as $QR$ is called the \emph{$QR$ decomposition} of $A$.
	\end{definition}

	\question
	Suppose $A,B,C$ are square matrices and $C=AB$.
	\begin{parts}
		\item How do the column spaces of $A$ and $C$ relate?
		\item How do the column spaces of $B$ and $C$ relate?
	\end{parts}

	\question
	$\mathcal V=\{\vec v_1,\vec v_2,\vec v_3\}$ forms a basis for $\R^3$.
	When we apply the Gram-Schmidt process to $\mathcal V$, we get
	\[
		\begin{array}{rl}
			q_1' &=\vec v\\
			q_2' &= \vec v_2-\frac{1}{2}\vec v_2\\
			q_3' &= \vec v_3-\vec v_1+2\vec v_2
		\end{array}
	\]
	form an orthogonal set.  Normalizing we get
	\[
		\begin{array}{rl}
			\vec q_1 &= 2q_1'\\
			\vec q_2 &= 3q_2'\\
			\vec q_3 &=\frac{1}{2}q_3'
		\end{array}
	\]
	form an orthonormal set.
	\begin{parts}
		\item Write $\vec v_1$ as a linear combination of $\vec q_1,\vec q_2,\vec q_3$.
		\item Write $\vec v_2$ as a linear combination of $\vec q_1,\vec q_2,\vec q_3$.
		\item Write $\vec v_3$ as a linear combination of $\vec q_1,\vec q_2,\vec q_3$.
	\end{parts}
	Define $A=[\vec v_1|\vec v_2|\vec v_2]$ and $Q=[\vec q_1|\vec q_2|\vec q_3]$.
	\begin{parts}[resume]
		\item Find a matrix $R$ so that $A=QR$.
	\end{parts}
	
	We've just discovered one process to find the $QR$ decomposition of a matrix.
	It's really as simple as doing Gram-Schmidt and keeping track of your coefficients.
	Now, we have another way to the matrix equation $A\vec x=\vec b$.  If we do a $QR$
	decomposition and exploit the fact that $Q^{-1}=Q^T$, we have
	\[
		A\vec x=QR\vec x=\vec b\qquad\implies\qquad R\vec x=Q^T\vec b
	\]
	and $R$ is a triangular matrix, so we can just do back substitution! (It turns
	out that if you solve systems this way, there is less rounding error than if you
	use row reduction.)


	\newpage
	\section*{Abstract Linear Algebra}
	\setcounter{question}{0}
	\begin{definition}[Vector Space]
		A non-empty set $V$ together with two functions ``$+$''$:V\times V\to V$
		and ``$\ \cdot\ $'':$\R\times V\to V$ is called a \emph{real vector space}
		if it satisfies the following axioms 
		\begin{enumerate}
			\item $(\vec a+\vec b)+\vec c=\vec a+(\vec b+\vec c)$ for all $\vec a,\vec b,\vec c\in V$
			\item $\vec a+\vec b=\vec b+\vec a$ for all $\vec a,\vec b\in V$
			\item There is an element $\vec 0\in V$ so that $\vec a+\vec 0=\vec a$ for all $\vec a\in V$
			\item For all $a\in V$, there is an element $-\vec a\in V$ so that $\vec a+(-\vec a)=\vec 0$
			\item $\alpha\cdot (\vec a+\vec b)=\alpha\cdot \vec a+\alpha\cdot \vec b$ for all $a,b\in V$ and $\alpha\in \R$
			\item $(\alpha+\beta)\cdot\vec a = \alpha\cdot \vec a+\beta\cdot \vec a$ for all $a\in V$ and $\alpha,\beta\in \R$
			\item $\alpha\cdot (\beta \cdot \vec a) = (\alpha\beta)\cdot \vec a$ for all $a\in V$ and $\alpha,\beta \in \R$
			\item $1\cdot \vec a=\vec a$ for all $a\in V$
		\end{enumerate}
	\end{definition}

	\question
	For each of the following sets, come up with a vector addition and scalar multiplication
	so that they form a vector space \emph{and} come up with a vector addition and scalar
	multiplication so that they don't form a vector space.
	\begin{parts}
		\item $A=\{\text{arrows in the plane}\}$.
		\item $T=\{\text{triples of real numbers}\}$.
		\item $F(\R)=\{\text{functions from $\R$ to $\R$}\}$.
		\item $R^+ = \{x\in\R:x>0\}$.
		\item $P_4=\{\text{polynomials of degree at most $4$}\}$.
		\item $M_{2\times 2}=\{2\times 2\text{ matrices with real entries}\}$.
	\end{parts}

	\begin{definition}[Subspace]
		Let $V$ be a vector space with operations ``$+$'' and ``$\ \cdot\ $''. A 
		\emph{subspace} of $V$ is a subset $W\subseteq V$ that is also a vector
		space with the vector addition and scalar multiplication induced
		by ``$+$'' and ``$\ \cdot\ $''.
	\end{definition}

	\question
		Let $V=\R^2$ be a vector space with component-wise addition and scalar
		multiplication.
	\begin{parts}
		\item Let $X\subseteq V$ be the $x$-axis. Is $X$ a subspace?
		\item Let $Y\subseteq V$ be defined by $Y=\{\vec r\in V:\vec r\in x\text{-axis or }
			\vec r\in y\text{-axis}\}$. Is $Y$ a subspace?
		\item Let $Z\subseteq V$ be defined by $Z=\{(x,y):x+y=0\}$.
	\end{parts}

	\newpage
	\begin{definition}[Linear Combination]
		The vector $\vec w$ is a\emph{linear combination} of the vectors $\vec v_1,\ldots,\vec v_n$
		if there are scalars $\alpha_1,\ldots,\alpha_n$ so that
		\[
			\vec w=\alpha_1\vec v_1+\cdots+\alpha_n\vec v_n.
		\]
	\end{definition}
	\begin{definition}[Span]
		Let $V$ be a vector space and let $X\subseteq V$ be a non-empty subset. We define the \emph{span}
		of $X$ to be the set
		\[
			\Span X = \{\vec x\in V:\text{$\vec x$ is a linear combination of vectors in $X$}\}.
		\]
		We define
		\[
			\Span \{\} = \{\vec 0\}.
		\]
	\end{definition}

	\question
		Identify the following definition of span as equivalent, inequivalent, or incoherent.
	\begin{parts}
		\item $\Span X = \{\vec x\in V : \vec x\text{ is in }X\}$
		\item $\Span X = \{\vec x\in V : \vec x=\sum\alpha_i\vec v_i\text{ for some }\vec v_i\in X\text{ and scalars }\alpha_i\}$
		\item $\Span X = \{\vec x\in V : \vec x=\sum\alpha_i\vec v_i\}$
		\item $\Span X = \{\vec x\in V : \vec x=\sum\alpha_i\vec v_i\text{ for all }\vec v_i\in X\text{ and scalars }\alpha_i\}$
		\item Let $\mathcal S = \{Y\subseteq V:X\subseteq Y\text{ and }Y\text{ is a subspace}\}$. 
			Then, $\displaystyle\Span X= \bigcap_{Y\in \mathcal S} Y$
		\item $\Span X = \displaystyle \bigcup_{\text{$\vec w$ is a linear combination of $X$}} \vec w$
		\item $\Span X = \displaystyle \bigcup_{\text{$\vec w$ is a linear combination of vectors in $X$}} \{\vec w\}$
	\end{parts}

	\question
		Let $V=P_2$ be the vector space of polynomials of degree at most $2$, and let $\vec u=x$, $\vec v=x^2$.
	\begin{parts}
		\item Describe $\Span\{\vec u\}$, $\Span\{\vec v\}$, and $\Span\{\vec u+\vec v\}$.
		\item Describe $\Span\{\vec u,\vec v\}$.
		\item Is $\Span\{\vec u,\vec v\}=V$? Explain.
	\end{parts}

	\question
	Let $V=\R^2$ and let $\vec u=\mat{1\\1}$ and $\vec v=\mat{1\\-1}$.
	\begin{parts}
		\item Prove $\Span\{\vec u,\vec v\}=V$.
		\item Prove $\Span\{\vec u\}\neq V$.
	\end{parts}

	\begin{definition}[Set Sum]
		Let $V$ be a vectors space. If $X,Y\subseteq V$, then the \emph{set sum}
		of $X$ and $Y$ is
		\[
			X+Y = \{\vec v\in V:\vec v=\vec x+\vec y\text{ for some }\vec x\in X\text{ and }\vec y \in Y\}.
		\]
	\end{definition}

	\question
		Let $V=\R^2$ and let $\vec u=\mat{1\\1}$ and $\vec v=\mat{1\\-1}$.
		\begin{parts}
		\item Describe $\{\vec u\}+\{\vec v\}$.
		\item Is $\{\vec u\}+\{\vec v\}=\{\vec u\}\cup\{\vec v\}$?
		\item Describe $\{\vec u\}+\Span\{\vec v\}$.
		\item Describe $\Span\{\vec u\}+\Span\{\vec v\}$.
		\item Is $\Span\{\vec u\}+\Span\{\vec v\} = \Span(\{\vec u\}+\{\vec v\})$? Explain.
		\end{parts}

	\begin{definition}[Linear Independence/Dependence]
		The vectors $\vec v_1,\ldots, \vec v_n$ are \emph{linearly independent} if
		\[
			\vec 0=\alpha_1\vec v_1+\cdots+\alpha_n\vec v_n
		\]
		implies $\alpha_1=\cdots=\alpha_n=0$ otherwise they are linearly
		dependent. A (possibly infinite) set of vectors is linearly dependent if it contains 
		$n$ linearly dependent vectors for some $n$.
	\end{definition}

	\question
		Identify the following statements about linear independence/dependence
		as equivalent to the definition, inequivalent to the definition, or incoherent.
	\begin{parts}
		\item $\vec v_1,\ldots,\vec v_n$ is linearly dependent if there is some $i$ for which
			\[
				\vec v_i \in \Span\{\vec v_1,\ldots,\vec v_{i-1},\vec v_{i+1},\ldots,\vec v_n\}.
			\]
		\item $\vec v_1,\ldots,\vec v_n$ is linearly dependent if there is some $i$ for which
			\[
				\vec v_i \in \Span\{\vec v_1,\ldots,\vec v_n\}.
			\]
		\item $\vec v_1,\ldots,\vec v_n$ is linearly dependent if $\vec v_1\in\Span\{\vec v_2,\ldots, \vec v_n\}$.
		\item $\vec v_1,\ldots,\vec v_n$ is linearly independent if the only linear combination is trivial.
	\end{parts}

	\question
	Prove the linear independence or dependence of the following sets.
	\begin{parts}
		\item $A=\left\{\mat{1\\0},\mat{0\\1}\right\}\subseteq \R^2$
		\item $B=\{1+x,x^2\}\subseteq P_2$
		\item $C=\Span\{\vec e_1\}\subseteq \R^3$
		\item $D=\{\vec 0\}\subseteq \R^n$
		\item $E=\{\}\subseteq \R^n$
	\end{parts}

	\newpage
	\begin{definition}[Basis]
		Let $V$ be a vector space. A \emph{basis} for $V$ is a subset $\mathcal B\subseteq V$ that
		satisfies
		\begin{enumerate}
			\item[(i)] $\mathcal B$ is linearly independent; and
			\item[(ii)] $\Span\mathcal B = V$.
		\end{enumerate}
	\end{definition}
	\begin{definition}[Dimension]
		Let $V$ be a vector space. The \emph{dimension} of $V$ is the size of a basis for $V$.
	\end{definition}

	\question
	Give two different bases for the following vector spaces.
	\begin{parts}
		\item $\R^3$
		\item $P_3$
		\item $M_{2\times 2}$
		\item Symmetric $2\times 2$ matrices.
		\item $R^+$, the set of positive reals, with operations defined
			by $[x]+[y]=[xy]$ and $\alpha[x]=[x^\alpha]$ for $[x],[y]\in R^+$ and $\alpha\in \R$.
	\end{parts}

	\begin{definition}[Well Defined]
		A object/concept is \emph{well defined} if its definition is unambiguous.
	\end{definition}

	\question
	For each of the following statements, decide if they provide a well-defined definition.
	\begin{parts}
		\item A real number $a$ is \emph{special} if $a^2>0$.
		\item The \emph{square root} of a positive real number $a$ is a number $b$ so
			that $b^2=a$.
		\item The \emph{square root} of a real number $a$ is a number $b$ so
			that $b^2=a$.
		\item The \emph{square root} of a real number $a$ is a positive number $b$ so
			that $b^2=a$.
		\item The \emph{dimension} of a vector space, $V$, is the number of elements in a basis
			for $V$.
	\end{parts}

	\question
	Consider the following theorem: If $\mathcal B$ is a basis for a vector space $V$, then every
	element in $V$ can be \emph{expressed uniquely} as a linear combination of vectors in $\mathcal B$.
	\begin{parts}
		\item Which part(s) of the definition of basis is needed to show every vector in $V$ can be
			\emph{expressed} as a linear combination of vectors in $\mathcal B$?
		\item Which part(s) of the definition of basis is needed to show that vectors 
			are \emph{uniquely} expressed as a linear 
			combination of vectors in $\mathcal B$?
		\item Prove the stated theorem.
	\end{parts}

	\newpage
	\begin{definition}[Linear Transformation]
		Let $V,W$ be a vector spaces. A function $T:V\to W$ is called
		\emph{linear} (or a \emph{linear transformation}) if
		\[
			T(\vec a+\vec b)=T\vec a+T\vec b\qquad\text{and}\qquad
			T(\alpha\vec b)=\alpha T\vec b
		\]
		for all $\vec a,\vec b\in V$ and scalars $\alpha$.
	\end{definition}

	\question
	For each transformation, decide whether it is linear or not.
	\begin{parts}
		\item $R:\R^2\to\R^2$ which reflects vectors across the line $y=x$.
		\item $G:\R^2\to\R^2$ which reflects vectors across the line $y=x+3$.
		\item $N:\R^2\to\R^2$ defined by $\mat{x\\y}\mapsto \vec 0$.
		\item $E:P_2\to P_2$ defined by $p\mapsto p(3)$.
		\item $M:R^+\to R^+$ defined by $[a]\mapsto [2a]$.
		\item $D:P_n\to P_{n-1}$ defined by $p\mapsto p'$ (where $p'$ is the derivative of $p$).
		\item $A:\R^n\to \R$ defined by $A\vec x=\text{average of coordinates of }\vec x$.
	\end{parts}

	\question
	A person is commuting to work. The function $\varphi_a(b)$ gives their distance from home
	$a+b$ minutes after midnight. You know $\varphi_0(t)=t(t-4)$.
	\begin{parts}
		\item Write down a formula for $\varphi_3(t)$.
		\item Let $T_1$ be defined so $T_1(\varphi_a) = \varphi_{a+1}$. Describe in words what $T$ does.
		\item Is $T$ linear?
		\item Let $R:\R^2\to\R^2$ be the function that translates all vectors left one unit.
			Is $R$ linear?
	\end{parts}

	\question
	\begin{parts}
		\item Suppose $T:V\to V$ is linear. Must $T(\vec 0) = \vec 0$? Prove your answer using definitions/axioms.
		\item Suppose $T:V\to V$ and $T(\vec 0)=\vec 0$. Must $T$ be linear? Prove your answer using definitions/axioms.
	\end{parts}

	\begin{definition}[Representation in a Basis]
		If $\mathcal B=\{\vec b_1,\ldots,\vec b_n\}$ is a basis for the vector space $\mathcal V$ and $\vec x\in\mathcal V$
		satisfies $\vec x=\alpha_1\vec b_1+\cdots+\alpha_n\vec b_n$, then the \emph{representation of $\vec x$ in
		the $\mathcal B$ basis} is the list of numbers
		\[
			[\vec x]_{\mathcal B} = \mat{\alpha_1\\\vdots\\\alpha_n}.
		\]
	\end{definition}

	\question
	Let $\mathcal B=\{\vec b_1,\vec b_2\}=\{\vec e_1+\vec e_2,\vec e_1-\vec e_2\}$ be a basis for $\R^2$ where $\mathcal S=
	\{\vec e_1,\vec e_2\}$ is the standard basis.
	\begin{parts}
		\item Is the representation of a vector in the $\mathcal B$ basis well defined? Why?
		\item Find $[\vec e_1]_{\mathcal B}$,\ \ $[2\vec e_1+\vec e_2]_{\mathcal B}$,\ \ $[\vec e_1]_{\mathcal S}$, and $[2\vec e_1+\vec e_2]_{\mathcal S}$.
		\item Consider the transformation $T:\R^2\to\R^2$ defined by $T(\vec v)=[\vec v]_{\mathcal B}$. Is $T$ linear?
	\end{parts}
	\question
	Let $\mathcal V$ be a vector space with bases $\mathcal A=\{\vec a_1,\vec a_2\}$ and
	$\mathcal B=\{\vec b_1,\vec b_2\}=\{\vec a_1+\vec a_2,\vec a_1-\vec a_2\}$.
	\begin{parts}
		\item Find $[\vec a_1]_{\mathcal B}$,\ \ $[2\vec a_1+\vec a_2]_{\mathcal B}$,\ \ $[\vec a_1]_{\mathcal A}$, and $[2\vec a_1+\vec a_2]_{\mathcal A}$.
		\item Consider the transformation $T:\mathcal V\to\R^2$ defined by $T(\vec v)=[\vec v]_{\mathcal B}$. Is $T$ linear?
	\end{parts}

	\newpage
	\question
	Let $\mathcal V$ be a vector space with bases $\mathcal A=\{\vec a_1,\vec a_2\}$ and
	$\mathcal B=\{\vec b_1,\vec b_2\}=\{\vec a_1+\vec a_2,\vec a_1-\vec a_2\}$. You know the following
	about the linear transformations
	$T,S:\mathcal V\to\mathcal V$.
	\[
		T\vec a_1=\tfrac{3}{2}\vec a_1+\tfrac{1}{2}\vec a_2\qquad 
		T\vec a_2=\tfrac{1}{2}\vec a_1+\tfrac{3}{2}\vec a_2\qquad\text{and}
		\qquad
		S \vec b_1 = 2\vec b_1\qquad
		S\vec b_2 = \vec b_2.
	\]
	\begin{parts}
		\item Compute $T(2\vec a_1+\vec a_2)$, $T(\vec a_1+\vec a_2)$, $S(2\vec a_1+\vec a_2)$, and $S(\vec a_1+\vec a_2)$.
		\item What information would you need to compute $T(\vec v)$ and $S(\vec v)$ for any vector in $\mathcal V$?
		\item Find a formula for $[T(\alpha\vec a_1+\beta\vec a_2)]_{\mathcal A}$ and $[S(\alpha\vec a_1+\beta\vec a_2)]_{\mathcal A}$.
		\item Find a formula for $[T(\alpha\vec a_1+\beta\vec a_2)]_{\mathcal B}$ and $[S(\alpha\vec a_1+\beta\vec a_2)]_{\mathcal B}$.
		\item Are $T$ and $S$ the same or different transformations?
		\item Can you find a matrix $X$ so that $X[\vec v]_{\mathcal A} = [T\vec v]_{\mathcal A}$? What
			about a matrix $Y$ so that $Y[\vec v]_{\mathcal B} = [T\vec v]_{\mathcal B}$?
		\item Are there any other matrices that might reasonably represent the transformations $T$ and $S$?
	\end{parts}

	\begin{definition}[Matrix Representation]
		Let $T:\mathcal V\to\mathcal W$ be a linear transformation between finite dimensional vector spaces;
		let $\mathscr A$ be a basis for $\mathcal V$ and let $\mathscr B$ be a basis for $\mathcal W$. 
		%Further,
		%define $A:\mathcal V\to\R^n$ and $B:\mathcal W\to\R^m$ by
		%\[
		%	A(\vec v) = [\vec v]_{\mathcal A}\qquad\text{and}\qquad B(\vec w)=[\vec w]_{\mathcal B}.
		%\]
		The \emph{matrix representation of $T$ with respect to the bases $\mathscr A$ and $\mathscr B$}
		is notated $[T]_{\mathscr A}^{\mathscr B}$ and is the matrix such that
		\[
			[T]_{\mathscr A}^{\mathscr B}[\vec v]_{\mathscr A} = [T(\vec v)]_{\mathscr B}.
		\]
	\end{definition}

	\question
	Let $T:\R^2\to\R^2$ be the transformation that reflects across the line $y=x$.
	\begin{parts}
		\item Find $[T]_{\mathcal S}^{\mathcal S}$ where $\mathcal S$ is the standard basis.
		\item Find a basis $\mathcal B$ so that $[T]_{\mathcal B}^{\mathcal B}$ is a diagonal matrix.
		\item Let $id:\R^2\to\R^2$ be the identity transformation. Find $[id]_{\mathcal S}^{\mathcal S}$,
		$[id]_{\mathcal B}^{\mathcal B}$, $[id]_{\mathcal B}^{\mathcal S}$, and $[id]_{\mathcal S}^{\mathcal B}$.
		\item What is the matrix product $[id]_{\mathcal B}^{\mathcal S}[id]_{\mathcal S}^{\mathcal B}$? Give
			a high-level explanation of this result (using the idea of bases).
	\end{parts}

	\begin{definition}[Matrix Multiplication]
		Let $T:\mathcal U\to\mathcal V$ and $S:\mathcal V\to\mathcal W$ and let $\mathscr A,\mathscr B,\mathscr C$
		be bases for $\mathcal U,\mathcal V,\mathcal W$, respectively. Let $P=[T]_{\mathscr A}^{\mathscr B}$ and
		$Q=[S]_{\mathscr B}^{\mathscr C}$. The \emph{matrix product} of $Q$ and $P$, written $QP$, is defined
		to be the matrix
		\[
			QP=[S\circ T]_{\mathscr A}^{\mathscr C}.
		\]
	\end{definition}

	\question
	Let $C$ be an $n\times n$ matrix and let $\mathscr A$ and $\mathscr B$ be bases for an $n$-dimensional vector
	space $\mathcal V$.
	\begin{parts}
	\item Use $C$ to define a linear transformation $T_C:\mathcal V\to\mathcal V$ (don't forget, you can use words
		in addition to symbols to specify the transformation).
		\item Under what conditions is it guaranteed that $C[id]_{\mathscr A}^{\mathscr B}=C$?
		\item Under what conditions is it guaranteed that $C[id]_{\mathscr A}^{\mathscr B}=I$, where $I$
			is the $n\times n$ identity matrix?
	\end{parts}

	\newpage
	\begin{definition}[One-to-one \& Onto]
		Let $f:A\to B$ be a function. $f$ is called \emph{one-to-one} (\emph{injective}) if
		$f(x)=f(y)$ implies $x=y$. $f$ is called \emph{onto} (\emph{surjective}) if for all
		$b\in B$ there is an $a\in A$ so that $f(a)=b$.
	\end{definition}

	\question
	\begin{parts}
		\item If possible, write down a one-to-one linear transformation with domain and codomain $\R^2$.
		\item If possible, write down an onto linear transformation with domain and codomain $\R^2$.
		\item If possible, write down a one-to-one linear transformation with domain $\R^2$ and codomain $\R^3$.
		\item If possible, write down an onto linear transformation with domain $\R^2$ and codomain $\R^3$.
		\item If possible, write down a one-to-one linear transformation with domain $\R^3$ and codomain $\R^2$.
		\item If possible, write down an onto linear transformation with domain $\R^3$ and codomain $\R^2$.
	\end{parts}

	\begin{definition}[Kernel \& Range]
		Let $T:\mathcal V\to\mathcal W$ be a linear transformation. The \emph{kernel} of $T$ is
		\[
			\Kern T = \{\vec x\in \mathcal V: T\vec x=\vec 0\}
		\]
		and the \emph{range} (or \emph{image}) of $T$ is
		\[
			\Range T = \{\vec x\in \mathcal W: \vec x=T\vec y\text{ for some }\vec y\in V\}.
		\]
	\end{definition}
	
	\question
		Let $\mathcal V$ and $\mathcal W$ be vectorspaces and let
		$T:\mathcal V\to\mathcal W$ be a linear transformation.
	\begin{parts}
		\item Prove that $\Kern T$ is a vector space.
		\item Prove that $\Range T$ is a vector space.
	\end{parts}

	\question
		Let $\mathcal V$ and $\mathcal W$ be vectorspaces with unknown dimensions and let
		$T:\mathcal V\to\mathcal W$ be a linear transformation.
	\begin{parts}
		\item If $\Kern T = \{\vec 0\}$ can you conclude whether or not $T$ is one-to-one?
			What about onto?
		\item If $\Kern T$ is three dimensional, can you conclude whether or not $T$ is one-to-one?
			What about onto?
		\item If $\Range T = \mathcal W$ can you conclude whether or not $T$ is one-to-one?
			What about onto?
		\item Is it possible that $\Kern T=\Range T$? Why or why not.
	\end{parts}

	\question
		Let $\mathcal V$ and $\mathcal W$ be vectorspaces and let
		$T:\mathcal V\to\mathcal W$ be a linear transformation.
	\begin{parts}
		\item If $\mathcal V$ and $\mathcal W$ are both two dimensional
			and $\Kern T=\{\vec 0\}$, can you conclude that $T$ is onto?
		\item If $\mathcal V$ is two dimensional and $\mathcal W$ is three dimensional
			and $\Kern T=\{\vec 0\}$, can you conclude that $T$ is onto?
		\item If $\mathcal V$ and $\mathcal W$ are both infinite dimensional
			and $\Kern T=\{\vec 0\}$, can you conclude that $T$ is onto?
	\end{parts}

	\begin{theorem}[Dimension Theorem]
		Let $T:\mathcal V\to\mathcal W$ be a linear transformation between finite dimensional
		vector spaces. Then
		\[
			\Dim(\Kern T) + \Dim(\Range T) = \Dim \mathcal V
		\]
	\end{theorem}
	\question
		The \emph{Rank-Nullity Theorem} says that for a matrix $M$, \[\nnul M + \Rank M = \#\text{cols in }M.\]
	\begin{parts}
		\item Explain how the Dimension Theorem and the Rank-Nullity theorem relate.
		\item How can you use your knowledge of matrix representations of linear transformations to 
			find $\Dim (\Kern T)$ or $\Dim (\Range T)$ for a linear transformation. Does a choice 
			of basis matter?
	\end{parts}


	\begin{definition}[Image \& Inverse Image]
		Let $T:\mathcal V\to\mathcal W$ be a function and let $A\subseteq \mathcal V$ and 
		$B\subseteq \mathcal W$ be sets. The \emph{image} of the set $A$ under $T$ is
		\[
			T(A) = \{\vec w\in\mathcal W:\vec w=T(\vec a)\text{ for some }\vec a\in A\}.
		\]
		The \emph{inverse image} of the set $B$ under $T$ is
		\[
			T^{-1}(B) = \{\vec v\in \mathcal V:T(\vec v)\in B\}.
		\]
	\end{definition}

	\question
		Let $S:\R\to\R$ be the squaring map $S(x)=x^2$.
	\begin{parts}
		\item What is $S(\R)$? How would you phrase this in the language of images/inverse images?
		\item What is $S^{-1}(\{2\})$? How would you phrase this in the language of images/inverse images?
		\item What is the difference between writing $S^{-1}(\{2\})$ and $S^{-1}(2)$? Is one more
			valid than the other?
		\item Let $T:\mathcal V\to\mathcal W$ be a linear transformation. Use images/inverse images
			to define the kernel of $T$ and the range of $T$.
	\end{parts}

	\question
		Let $M=\mat{1&2\\2&4}$ and define $T:\R^2\to\R^2$ by $\vec x\mapsto M\vec x$.
	\begin{parts}
		\item Is $T$ one-to-one?
		\item Describe $\Range T$.
		\item Compute $T^{-1}(\{\mat{3\\6}\})$.
		\item For $\vec v\in\Range T$, consider the set $X_{\vec v}=T^{-1}(\{\vec v\})$. What does
			$X_{\vec v}$ look like as $\vec v$ changes? Can you describe $X_{\vec v}$ using ideas
			like kernel or range?
	\end{parts}

	\question
		Let $T:\mathcal V\to\mathcal W$ be a linear transformation and let $\vec v\in\mathcal V$ and
		$\vec w\in\mathcal W$ so that $T(\vec v)=\vec w$.
	\begin{parts}
		\item Prove $T^{-1}(\{\vec w\}) = \{\vec v\}+\Kern T$.
		\item Is $T^{-1}(\{\vec w\})$ a subspace? What about $T^{-1}(\Span\{\vec w\})$?
	\end{parts}

	\question
		Let $T:\mathcal V\to\mathcal W$ be an onto linear transformation and let $\mathcal U\subseteq \mathcal W$
		be a subspace.
	\begin{parts}
		\item Prove that $T^{-1}(\mathcal U)$ is a subspace.
		\item What can you say about the dimension of $T^{-1}(\mathcal U)$ and
			the dimension of $\mathcal U$?
		\item Let $S:\mathcal W\to\mathcal Q$. What can you say about the kernel of
			$S\circ T$? What can you say about its dimension?
	\end{parts}

	\begin{definition}[Inverse]
		Let $T:\mathcal V\to\mathcal W$ be a function. $T$ is
		\emph{invertible} if there exists a function $S:\mathcal W\to\mathcal V$ so that
		\[
			S\circ T=id_{\mathcal V}\qquad\text{and}\qquad T\circ S=id_{\mathcal W}
		\]
		where $id_{\mathcal V}$ is the identity function on $\mathcal V$ and $id_{\mathcal W}$
		is the identity function on $\mathcal W$.
		If $S$ exists, we notate it by $T^{-1}$.
	\end{definition}

	\question
	Let $\mathcal V$ be a finite-dimensional vector space
	with a basis $\mathscr A$. Let $T:\mathcal V\to\mathcal V$ be a linear transformation.
	\begin{parts}
		\item Show that $T$ is invertible if and only if $[T]_{\mathscr A}^{\mathscr A}$ is invertible.
		\item Let $\mathscr B$ and $\mathscr C$ be bases for $\mathcal V$. Show that $T$ is invertible if
			and only if $[T]_{\mathscr B}^{\mathscr C}$ is invertible.
		\item If $T$ is invertible, what can you say about the kernel of $T$.
	\end{parts}

	\newpage

	\question
	Let $\vec u=\mat{1\\0\\0}$ and $\vec v=\mat{1\\1\\0}$ and $\mathscr B=\{\vec u,\vec v\}$
	and let $\mathscr E=\{\vec e_1,\vec e_2\}$ be the standard basis for $\R^2$. Define $\mathcal V=\Span \mathscr B\subseteq \R^3$ 
	and consider
	the linear transformation 
	\[
		T:\mathcal V\to\R^2\qquad\text{where}\qquad \vec x\mapsto \mat{1&0&0\\0&1&0}\vec x.
	\]
	\begin{parts}
		\item Is $\vec x=\mat{4\\1\\0}$ in the domain of $T$? If so, compute $T(\vec x)$.
		\item Is $\vec y=\mat{4\\1\\1}$ in the domain of $T$? If so, compute $T(\vec y)$.
		\item Write $[\vec x]_{\mathscr B}$ and $[\vec y]_{\mathscr B}$, if possible.
		\item Is $T$ invertible?
		\item What is $[T]_{\mathscr B}^{\mathscr E}$? Does $[T]_{\mathscr B}^{\mathscr E}=\mat{1&0&0\\0&1&0}$?
			Are there other bases $\mathscr C$ and $\mathscr D$ so that $[T]_{\mathscr C}^{\mathscr D}=\mat{1&0&0\\0&1&0}$?
			Why or why not?

	\end{parts}

	\begin{definition}[Isomorphic]
		The vector spaces $\mathcal V$ and $\mathcal W$ are called \emph{isomorphic}
		if there exists an invertible linear transformation $\Phi:\mathcal V\to\mathcal W$.
		In this case, $\Phi$ is called a \emph{vectorspace isomorphism}.
	\end{definition}
	\question
	\begin{parts}
		\item Identify three distinct subspaces of $\R^3$ which are isomorphic to $\R^2$.
		\item Let $\mathscr B$ be a basis for the $n$-dimensional vectorspace $\mathcal W$.
			Give an isomorphism between $\mathcal W$ and $\R^n$.
		\item Prove or disprove: If $\mathcal V$ and $\mathcal W$ are isomorphic, then
			they have the same dimension.
		\item Prove or disprove: If $\mathcal V$ and $\mathcal W$ are real vectorspaces
			with the same dimension, then they are isomorphic.
	\end{parts}


	\begin{definition}[Push-forward \& Conjugation]
		Let $\Phi:\mathcal V\to\mathcal W$ be an isomorphism of vector spaces
		and let $T:\mathcal V\to\mathcal V$ be a linear transformation. The 
		\emph{push-forward} of $T$ under $\Phi$ is the linear transformation
		$\Phi\circ T\circ \Phi^{-1}:\mathcal W\to\mathcal W$.

		Two linear transformations are called \emph{conjugate} if one is a push-forward
		of the other. Two matrices are called \emph{similar} if they correspond to
		conjugate linear transformations.
	\end{definition}

	\question
	Consider the linear transformations $T_A$, $T_B$, etc., defined by multiplication
	by the following matrices.
	\[
		A=\mat{1&0\\0&1}\qquad B=\mat{1&1\\1&1}\qquad C=\mat{2&2\\2&2}
	\]
	\[
		E=\mat{2&0\\0&0}\qquad F=\mat{0&0\\0&2}\qquad G=\mat{1&1\\1&-1}
	\]
	\begin{parts}
		\item Compute the dimension of the range and kernel of each transformation $T_A$, $T_B$, etc..
		\item Of $T_A$, $T_B$, etc., are there any pairs that you can
			tell aren't conjugate? How do you know?
		\item Can conjugate transformations have different kernels? Can the dimension of their
			kernels differ? Prove your answer.
		\item If two transformations have the same kernel, must they be conjugate? What if just the dimension
			of their kernels are the same?
	\end{parts}
	
	\question
		Let $\mathscr A$ and $\mathscr B$ be bases for $\R^2$.
			Let $T:\R^2\to\R^2$ be a linear transformation
			and consider the transformations
			$M_A,M_B:\R^2\to\R^2$ defined by multiplication by the matrices $A=[T]_{\mathscr A}^{\mathscr A}$
			and $B=[T]_{\mathscr B}^{\mathscr B}$.
	\begin{parts}
		\item Are $M_A$ and $M_B$ conjugate? If so, give a conjugacy.
		\item Are $A$ and $B$ similar matrices? Explain.
		\item Your textbook defined the matrices $X$ and $Y$ to be similar if there is an invertible
			matrix $Q$ so that $X=QYQ^{-1}$. How does this definition relate
			to the definition in terms of conjugacies?
	\end{parts}

	\begin{definition}[Conjugacy Invariant]
		A property, $\mathscr P$, of a transformation/matrix is called
		a \emph{conjugacy invariant} if
		\[
			\mathscr P(T)=\mathscr P(S)
		\]
		whenever $T$ and $S$ are conjugate.
	\end{definition}

	\question
	Consider the following properties
	\[
		\mathscr D(T)=\text{ dimension of the domain of $T$}
		\qquad
		\mathscr R(T)=\text{ dimension of the range of $T$}
	\]
	\[
		\mathscr K(T)=\text{ kernel of $T$}
		\qquad
		\mathscr N(T)=\text{ dimension of the kernel of $T$}
	\]
	\[
		\mathscr V(T)=\begin{cases}1&\text{ if $\mat{1\\1}$ is an eigenvector for $T$}\\0& \text{ else}\end{cases}
			\qquad
		\mathscr E(T)=\begin{cases}
			1&\text{if $3$ is an eigenvalue for $T$}\\0&\text{ else}
		\end{cases}
	\]
	\begin{parts}
		\item For each property, decide whether it is a conjugacy invariant.
		\item Do any of the listed properties \emph{determine} if two transformations
			are conjugate? That is, if $\mathscr P(T)=\mathscr P(S)$ is it guaranteed
			that $T$ and $S$ are conjugate?
	\end{parts}

	\subsection*{Eigenstuff}

	\begin{definition}[Eigenvalues \& Eigenvectors]
		Let $T:\mathcal V\to\mathcal V$ be a linear transformation. A non-zero
		vector $\vec v\in\mathcal V$ is called an \emph{eigenvector} for $T$
		if
		\[
			T\vec v=\lambda \vec v
		\]
		for some scalar $\lambda$. The scalar $\lambda$ is called the \emph{eigenvalue}
		associated with $\vec v$.

		Given an eigenvalue $\lambda$, the \emph{eigenspace} with eigenvalue $\lambda$
		is the span of all eigenvectors with eigenvalue $\lambda$.
	\end{definition}

	\question
	Let $P:\R^3\to\R^3$ be projection onto the $xy$-plane and let $Q:\R^3\to\R^3$ be projection
	onto the line $\ell=\Span\{\xh+\yh+\zh\}$.

	\begin{parts}
		\item Find all eigenvectors and eigenvalues for $P$ and $Q$.
		\item Find all eigenspaces for $P$ and $Q$.
		\item Let $A,B$ be eigenspaces for $P$ with different eigenvalues.
			What can you say about $A\cap B$? What about $A+B$? Will
			these conclusions hold for transformations other than $P$?
	\end{parts}

	\newpage
	\question
	Let $\mathscr E=\{\xh,\yh,\zh\}$ and $\mathscr B=\{\mat{1\\2\\0},\yh,\zh\}$.
	\begin{parts}
		\item Write down a linear transformation $T:\R^3\to\R^3$ where $\mathscr E$ are eigenvectors
			with eigenvalues $2,3,4$.
		\item Write down a linear transformation $S:\R^3\to\R^3$ where $\mathscr B$ are eigenvectors
			with eigenvalues $2,3,4$.
		\item Are $T$ and $S$ conjugate? Why or why not?
	\end{parts}

	\begin{definition}[Characteristic Polynomial]
		Let $T:\mathcal V\to\mathcal V$ be a linear transformation and let $\mathscr B$ be a basis for 
		the finite-dimensional vector space $\mathcal V$. The \emph{characteristic
		polynomial} of $T$ is 
		\[
			\Char(T) = p(x) =\det([T]_{\mathscr B}^{\mathscr B} - xI).
		\]
	\end{definition}

	\question
	\begin{parts}
		\item What needs to be checked to determine if the characteristic polynomial
			of a linear transformation is well defined?
		\item Prove that $\Char(A)$ is well defined for a linear transformation $A:\mathcal V\to\mathcal V$.
		\item Is $\Char$ a conjugacy invariant?
		\item Explain how $\Char(A)$ relates to the eigenvalues or eigenvectors of $A$.
	\end{parts}

	\question
	Let $X=\Span\{1,e^x,e^{2x}\}$.
	Let $D_P:P_2\to P_2$ be the derivative operator on $P_2$ and let $D_X:X\to X$ be the derivative
	operator on $X$.
	\begin{parts}
		\item Find $\Char(D_P)$ and $\Char(D_X)$.
		\item Find all eigenvalues for $D_P$ and $D_X$.
		\item Find all eigenspaces for $D_P$ and $D_X$.
	\end{parts}

	\begin{definition}[Geometric \& Algebraic Multiplicity]
		Let $T:\mathcal V\to\mathcal V$ be a linear transformation 
		between finite-dimensional vector spaces and let $\lambda$
		be an eigenvalue for $T$. The \emph{geometric multiplicity} of $\lambda$ is
		the dimension of the associated eigenspace. The \emph{algebraic multiplicity} of
		$\lambda$ is the number of times $\lambda$ appears as a root of $\Char(T)$.
	\end{definition}

	\question
	Let $D_P$ and $D_X$ be derivative operators on $P_2$ and $X=\Span\{1,e^x,e^{2x}\}$, respectively (as before).
	\begin{parts}
		\item Find the geometric and algebraic multiplicities for each eigenvalue of $D_P$ and $D_X$.
		\item Does $X=E_0+E_1+E_2$ where $E_0$, $E_1$, and $E_2$ are eigenspaces for $D_X$?
		\item Can $P_2$ be written as the sum of eigenspaces for $D_P$?
		\item Make a conjecture to complete the following sentence: 
			``Let $\mathcal V$ be a finite-dimensional vector space and let $T:\mathcal V\to\mathcal V$
			be a linear transformation. $\mathcal V$
			can be written as a sum of eigenspaces for $T$ exactly when \ldots''.
	\end{parts}

	\newpage
	\question
	Let $T:\mathcal V\to\mathcal V$ be a linear transformation with eigenspaces
	\[
		E_1=\Span\{\vec a,\vec b\}\qquad E_3=\Span\{\vec c,\vec d\}\qquad E_{-5}=\Span\{\vec e\}
	\]
	with eigenvalues $1,3$, and $-5$ and dimensions $2$, $2$, $1$, respectively. Further, suppose $\mathcal V=E_1+E_3+E_{-5}$.
	\begin{parts}
		\item Compute $T(\vec a+2\vec d)$.
		\item Can you determine whether $\{\vec a,\vec c\}$ linearly independent or dependent?
			If so, which is it?
		\item Are you justified in writing $\mathcal V=E_1\oplus E_3\oplus E_{-5}$? Why or why not?
		\item Find a matrix representation for $T$.
	\end{parts}

	\question
	Consider the following table.

	\begin{tabular}{c|c|c|c|c}
		Transformation & Eigenvalues & Geom. Mult. & Alg. Mult. & Dim(Domain)\\
		\hline
		A & 1,2,3 & 1,1,1 & 1,1,1 & 3\\
		B & 1,2,3 & 1,1,1 & 1,1,1 & 3\\
		C & 1,2 & 2,1& 2,1 & 3\\
		D & 1,2 & 2,1& 2,1 & 3\\
		E & 1,2 & 2,1& 5,2 & 7\\
		F & 1,2 & 2,1& 5,2 & 7\\
	\end{tabular}

	\begin{parts}
		\item Which transformations are diagonalizable?
		\item Which transformations can you tell are \emph{not} conjugate?
		\item Which transformations can you tell \emph{are} conjugate?
		\item Are there any pairs of transformations where you don't have enough
			information to tell if they are conjugate or not?
	\end{parts}


	\newpage
	\subsection*{Fields}

	\question
	Let $T_J,T_B:\R^2\to\R^2$ be given by multiplication by the matrices
	$J=\mat{0&-1\\1&0}$ and $B=\mat{0&-2\\2&0}$.
	\begin{parts}
		\item Do $T_J$ or $T_B$ have eigenvectors? Why or why not?
		\item Are $T_J$ and $T_B$ conjugate?
	\end{parts}

	\begin{definition}[Complex Numbers]
		The \emph{complex numbers}, denoted by $\C$, are the real vector-space
		$\R^2$ equipped with the bi-linear multiplication
		\[
			\mat{a\\b}\star\mat{c\\d} = \mat{ac-bd\\ad+bc}.
		\]
		The standard basis for $\C$ is commonly written as $\{1,i\}$
		(or $\{1,j\}$ if you're an engineer).
	\end{definition}

	\question
	\begin{parts}
		\item Express $\mat{2\\3},\mat{-1\\2}\in\C$ in the standard basis for
			$\C$ and compute $\mat{2\\3}\star\mat{-1\\2}$.
		\item Let $R=\Span\{1\}\subseteq \C$. Show that $R$ is closed under
			``$\star$''.
		\item Would we be justified in calling $R$ and $\R$ isomorphic? Why or why not?
	\end{parts}

	\question
	Let $p(x)=x^2+1$ and let $q(x)=x^4+1$. 
	\begin{parts}
		\item If possible, find all roots of $p$ in $\C$.
		\item Find all solutions to the equation $i\star x=1$. Can you make sense
			of the expression $1/i$?
		\item Let $u=\sqrt{2}/2+(\sqrt{2}/2)i$ and define the transformation
			$T_u:\C\to \C$ by $v\mapsto u\star v$. Is $T$ linear? Can you describe it geometrically?
		\item Can you find a root of $q$ in $\C$?
	\end{parts}

	\question
	Let $u=a+bi\in \C$ be a fixed, and define the linear transformation $T_u:\C\to\C$
	by $v\mapsto u\star v$.
	\begin{parts}
		\item Find a matrix for $T_u$.
		\item Under what conditions is $T_u$ invertible?
		\item For $\alpha\in\C$, under what conditions does the equation
			$u\star x=\alpha$ have a solution?
		\item Would we be justified in writing $1/\alpha$ for a complex number
			$\alpha$? Why or why not?
	\end{parts}

	\newpage
	\begin{definition}[Field]
		A set $F$ coupled with two operations $+:F\times F\to F$
		and $\star:F\times F\to F$ is called a \emph{field}
		if it satisfies the following axioms.
		\begin{enumerate}
			\item $a+(b+c)=(a+b)+c$ and $a\star(b\star c)=(a\star b)\star c$ for all $a,b,c\in F$
			\item $a+b=b+a$ and $a\star b=b\star a$ for all $a,b\in F$
			\item $a\star(b+c)=(a\star b)+(a\star c)$ for all $a,b,c\in F$
			\item There exists an element $0\in F$ satisfying $a+0=a$ and $a\star 0=0$ for all
				$a\in F$
			\item For all $a\in F$, there exists $-a\in F$ so that $a+(-a)=0$
			\item There exists $1\in F$ so that $1\star a=a$ for all $a\in F$.
			\item For all $a\in F$ satisfying $a\neq 0$, there exists
				$a^{-1}\in F$ so that $a\star a^{-1}=1$.
		\end{enumerate}
	\end{definition}

	\question
	Consider the following sets:
	$
		\qquad\R\qquad R^+=\{x\in R:x>0\}\qquad \Q\qquad \Z\qquad \C.
	$
	\begin{parts}
		\item For each set, decide whether it is a field.
		\item For each field, if possible, come up with (i) a polynomial that has a root
			and (ii) a polynomial that doesn't have a root.
	\end{parts}
	\begin{definition}[Algebraically Closed Field]
		A field $F$ is called \emph{algebraically closed} if every non-constant polynomial with
		coefficients in $F$ has a root in $F$.
	\end{definition}

	\question
	\begin{parts}
		\item For the fields $\Q$, $\R$, and $\C$, decide if they are algebraically closed.
		\item Find a linear transformation $T:\R^2\to\R^2$ that has no eigenvectors.
		\item Find a linear transformation $S:\R^2\to\R^2$ with the following properties:
			(i) $S(\Q^2)\subseteq \Q^2$, (ii) $S$ has eigenvectors, and (iii)
			$R=S\big|_{\Q}$ has no eigenvectors.
		\item Can you find a linear transformation $U:\C^2\to\C^2$ which has no eigenvectors?
	\end{parts}

	\question
	\begin{parts}
		\item Let $\mathcal V=\Span_{\Q}\{1,\sqrt{2}\}\subseteq \R$ be a $\Q$ vector space. What dimension
			is $\mathcal V$?
		\item Let $\mathcal W=\Span_{\Q}\{1,\sqrt{2},\sqrt{3},\sqrt{4}\}\subseteq \R$ be a $\Q$ vector space. What dimension
			is $\mathcal W$?
		\item Consider $\mathcal R=\R$ as a $\Q$ vector space. What dimension is $\mathcal R$?
	\end{parts}

	\newpage
	\subsection*{Jordan Forms}
	\begin{definition}[Invariant Subspace]
		Let $T:\mathcal V\to\mathcal V$ be a linear transformation. The subspace
		$\mathcal W \subseteq \mathcal V$ is called an \emph{invariant subspace} with
		respect to $T$ if $T(\mathcal W)\subseteq \mathcal W$.
	\end{definition}

	\question
	Let $P:\R^3\to\R^3$ be projection onto the $xy$-plane and let $D:P_2\to P_2$ be
	differentiation.
	\begin{parts}
		\item Find two invariant subspaces for $P$ and two non-invariant subspaces
			for $P$.
		\item Find two invariant subspaces for $D$ and two non-invariant subspaces
			for $D$.
	\end{parts}

	\question
	\begin{parts}
		\item Is the property ``$\mathcal W$ is an invariant subspace'' a conjugacy invariant?
		\item Is the property ``there is an invariant subspace of dimension $k$'' a conjugacy invariant?
		\item Prove or disprove: every eigenspace is an invariant subspace.
		\item Prove or disprove: every invariant subspace is an eigenspace.
	\end{parts}

	\question
	Let $\mathscr E$ be the standard basis for $\R^3$ and let $T:\R^3\to\R^3$ be defined by 
	\[
		[T]_{\mathscr E}^{\mathscr E}=\mat{2&1&-1\\0&1&2\\0&0&1}.
	\]
	\begin{parts}
		\item Is $A=\Span\{\vec e_1\}$ an invariant subspace?
		\item Find an invariant subspace of dimension two, if possible.
		\item Find a chain of invariant subspaces $\{\vec 0\}=S_0\subsetneq S_1\subsetneq S_2\subsetneq S_3=\R^3$.
			Are there multiple possibilities for this chain or is it unique?
		\item Produce a linear transformation $Q:\R^4\to\R^4$ where $X=\Span\{\vec e_1,\vec e_2\}$
			is an invariant subspace, but $Y=\Span\{\vec e_1,\vec e_2,\vec e_3\}$ is not.
	\end{parts}

	\begin{definition}[Cyclic Subspace]
		Let $T:\mathcal V\to\mathcal V$ be a linear transformation and let $\vec x\in \mathcal V$.
		The \emph{cyclic susbspace generated by $\vec x$} is denoted $C(\vec x)$ and defined to be
		\vspace{-.3cm}
		\[
			C_T(\vec x) = \Span\{\vec x,T\vec x,T^2\vec x,T^3\vec x,\ldots\}.
		\]

		\vspace{-.3cm}
		A subspace $\mathcal W\subseteq \mathcal V$ is called \emph{cyclic} with respect to $T$ 
		if there exists $\vec x\in\mathcal V$ so that $\mathcal W=C_T(\vec x)$.
	\end{definition}

		\vspace{-.3cm}
	\question
		\vspace{-.2cm}
	Let $R:\R^2\to\R^2$ be rotation counter-clockwise by $90^\circ$ and let $S:\C^2\to\C^2$ be defined
	by $\mat{x\\y}\mapsto\mat{-y\\x}$.
	\begin{parts}
		\item Find all cyclic subspaces of $\R^2$ with respect to $R$.
		\item Find all cyclic subspaces of of $\C^2$ with respect to $S$.
		\item Is every eigenspace a cyclic subspace?
		\item Under what conditions is an eigenspace a cyclic subspace?
	\end{parts}

	\begin{definition}[Nilpotent Map]
		A linear transformation $T:\mathcal V\to\mathcal V$ is called \emph{nilpotent}
		if there is some $n<\infty$ so that $T^n\vec x=\vec 0$ for all $\vec x\in\mathcal V$.
		In other words, $T^n$ is the zero transformation.
	\end{definition}

	\question
	Let $T:\mathcal V\to\mathcal V$ be a nilpotent map.
	\begin{parts}
		\item What are the eigenvalues of $T$?
		\item What is the smallest $n$ that guarantees 
			that $T^n=0$?
		\item Suppose $T$ is diagonalizable. What is the smallest $n$ that guarantees 
			that $T^n=0$?
	\end{parts}

	\newpage
	\question
	Let $\mathscr E$ be the standard basis for $\R^5$ and define $T:\R^5\to\R^5$ by
	\[
		[T]_{\mathcal E}^{\mathcal E}=\mat{0&1&0&0&0\\0&0&0&0&0\\0&0&0&1&0\\0&0&0&0&1\\0&0&0&0&0}
	\]
	\begin{parts}
		\item Is $T$ nilpotent? If so, find the smallest $n$ so that $T^n=0$.
		\item Suppose $\mathcal W$ cyclic subspace of $T$. What can you say about the dimension of $\mathcal W$?
		\item Can you find cyclic subspaces $C_1,\ldots, C_k$ so that $\R^5=C_1\oplus C_2\oplus\cdots\oplus C_k$?
	\end{parts}

	\begin{theorem}[Canonical Representation of Nilpotent Maps]
		Let $T:\mathcal X\to\mathcal X$ and $S:\mathcal Y\to\mathcal Y$ be nilpotent
		maps on finite-dimensional vector spaces. $T$ and $S$ are conjugate if and
		only if $\mathcal X$ and $\mathcal Y$ have the same decomposition into
		cyclic subspaces.  That is, there exist $\vec x_1,\ldots, \vec x_k\in \mathcal X$
		and $\vec y_1,\ldots,\vec y_k\in \mathcal Y$ so that
		\[
			\mathcal X=C_T(\vec x_1)\oplus \cdots \oplus C_T(\vec x_k)\qquad\text{and}
			\qquad
			\mathcal Y=C_S(\vec y_1)\oplus \cdots \oplus C_S(\vec y_k)
		\]
		and 
		\[
			\Dim(C_T(\vec x_i))=\Dim(C_S(\vec y_i))\qquad\text{ for }i=1,\ldots, k.
		\]
	\end{theorem}

	\question
	We will prove most of this theorem (CRNM) little-by-little.
	\begin{parts}
		\item Is it enough to prove CRNM for two maps $T,S:\mathcal V\to\mathcal V$ (instead of letting
		the maps have different domains)? Why or why not?
		\item Let $T:\mathcal V\to\mathcal V$ be nilpotent and let $\vec x\in \mathcal V$.
			Suppose that $\mathcal V=C_T(\vec x)$. Write down a basis for $\mathcal V$.
		\item Suppose $T,S:\mathcal V\to\mathcal V$ are nilpotent and $\mathcal V=C_T(\vec x)=C_S(\vec y)$
			for some $\vec x,\vec y\in \mathcal V$. Prove that $T$ and $S$ are conjugate.
		\item Suppose $T,S:\mathcal V\to\mathcal V$ are nilpotent and 
			\[
				\mathcal V=C_T(\vec x_1)\oplus\cdots\oplus C_T(\vec x_k)=C_S(\vec y_1)\oplus
				\cdots \oplus C_S(\vec y_k)
			\]
			for some $\vec x_1,\ldots,\vec y_1,\ldots \in \mathcal V$. Further suppose $\Dim(C_T(\vec x_i))=
			\Dim(C_T(\vec y_i))$ for all $i$. Prove $T$ and $S$ are conjugate.
		\item Suppose $T,S:\mathcal V\to\mathcal V$ are nilpotent and conjugate. Further
			suppose $\mathcal V=C_T(\vec x_1)\oplus \cdots C_T(\vec x_k)$ for some $\vec x_1,\ldots,\vec x_k\in\mathcal V$.
			Prove that $\mathcal V$ can be decomposed into $S$-cyclic subspaces in the same way.
		\item Just for fun, suppose $T,S:\mathcal V\to\mathcal V$ are nilpotent and $\mathcal V=C_T(\vec x)=C_S(\vec y_1)\oplus C_S(\vec y_2)$
			for some non-zero $\vec x,\vec y_1,\vec y_2\in\mathcal V$. Prove that $T$ and $S$ are not conjugate.
		\item Have we proved the theorem? Why or why not?
	\end{parts}

	\newpage
	\begin{theorem}[Decomposition Theorem]
		Let $\mathcal V$ be finite dimensional let $T:\mathcal V\to\mathcal V$ be a nilpotent map.
		Then, $\mathcal V$ can be decomposed into cyclic subspaces.
	\end{theorem}

	\question
		Let $\mathcal V$ be a finite-dimensional vector space and let $T:\mathcal V\to\mathcal V$
		be a nilpotent map so that $T^{n+1}=0$ but $T^{n}\neq 0$.
	\begin{parts}
		\item Does there exist an $\vec x\in \mathcal V$ so that $\Dim(C(\vec x))=n+1$? Does there exist a $\vec y\in\mathcal V$
			so that $T(\vec y)=\vec x$?
		\item Show that if $\mathcal Y$ is invariant, then $\mathcal Y+C(\vec w)$ is invariant for any $\vec w$.
		
		\item Fix $\vec x$ such that $\Dim(C(\vec x))=n+1$ and fix $\vec y\in\mathcal V$. Suppose $T^a\vec y\in C(\vec x)$
			but $T^{a-1}\vec y\notin C(\vec x)$,
			and express $T^a\vec y$ as  \[T^a\vec y=\alpha_0\vec x+\alpha_1T\vec x+\cdots \alpha_n T^n\vec x.\]
			Is it possible that $\alpha_0\neq 0$? Can you
			draw any conclusions about any other $\alpha_i$?
		\item Fix $\vec x$ and $\vec y$ as before. Let \[
				\vec w=\vec y-\alpha_a \vec x-\alpha_{a+1} T\vec x -\cdots -\alpha_{n}T^{n-a}\vec x.
			\]
			Show that $C(\vec x)\cap C(\vec w)=\{\vec 0\}$.
		\item Let $\vec x$ be as before and suppose $\mathcal Y$ is an invariant 
			subspace and $C(\vec x)\oplus \mathcal Y\subsetneq \mathcal V$.
			Show that there is a larger invariant subspace $\mathcal Y\subsetneq \mathcal Y'$ so
			that $C(\vec x)\oplus \mathcal Y'=\mathcal V$.
		\item Prove the decomposition theorem.
	\end{parts}

	\begin{definition}[Jordan Block]
		A \emph{Jordan block} of size $n$ and value $\lambda$ is an $n\times n$ matrix of the form
		\[
			J^n(\lambda)=\mat{\lambda &1&0&\cdots &0\\
			     0&\lambda &1&\cdots&0\\0& 0 &\lambda &\cdots&0\\
			     \vdots &\vdots &\vdots &\ddots&\vdots\\
			     }
		\]
		with $\lambda$ on the diagonal and $1$ on the super-diagonal.
	\end{definition}

	\question
	\begin{parts}
		\item The matrix $M$ has Jordan blocks $J^2(5)$ and $J^3(\pi)$ on the diagonal
		and zeros everywhere else. Write down $M$.
		\item The nilpotent map $T:\mathcal V\to\mathcal V$ has a decomposition
			into cyclic subspaces of dimensions $2$, $2$, and $3$.
			Write down a matrix $B$ with Jordan blocks on its diagonal which
			is similar to $[T]_{\mathscr B}^{\mathscr B}$ for any choice of basis $\mathscr B$.
	\end{parts}

	\begin{theorem}[Caley Hamilton]
		Let $M$ be an $n\times n$ matrix with characteristic polynomial $p(x)$.
		Then $p(M)=0$.
	\end{theorem}

	\question
	\begin{parts}
		\item Verify Caley Hamilton for $A=\mat{1&0\\0&2}$.
		\item Verify Caley Hamilton for $B=\mat{0&-1\\1&0}$.
	\end{parts}

	\question
	Let $\mathcal V$ be a three-dimensional vector space over an algebraically closed
	field and let $T:\mathcal V\to\mathcal V$
	be a linear transformation with a single eigenvalue $\lambda$.
	\begin{parts}
		\item Write down $\Char(T)$.
		\item Show that $T-\lambda id$ is nilpotent.
		\item List all possible Jordan block structures that $T$ could have.
	\end{parts}

	\newpage
	\begin{definition}[Generalized Eigenspace]
		Let $T:\mathcal V\to\mathcal V$ be a linear transformation and let $\lambda$
		be an eigenvalue for $T$. The \emph{generalized eigenspace} with eigenvalue $\lambda$
		is
		\[
			\Eig_\lambda(T) = \bigcup_{n>0} \Kern\big((T-\lambda id)^n\big).
		\]
	\end{definition}

	\question
		Let $T:\R^5\to\R^5$ have matrix $M=\mat{2&1&0&0&0\\
			    0&2&0&0&0\\
			    0&0&3&1&0\\
			    0&0&0&3&0\\
			    0&0&0&0&3}$
	\begin{parts}
		\item Find all eigenvalues, eigenspaces, algebraic multiplicities, and geometric multiplicities of 
			eigenvalues for $T$.
		\item Find all generalized eigenspaces for $T$.
		\item Are the generalized eigenspaces of $T$ invariant?
		\item Is $M$ diagonalizable?
		\item Is $\R^5$ decomposable into the direct sum of eigenspaces? What about generalized eigenspaces?
	\end{parts}

	\begin{theorem}
		Let $\mathcal V$ be finite dimensional and let $T:\mathcal V\to\mathcal V$ be a linear transformation.
		Then $\Dim(\Eig_\alpha(T)) = \text{ algebraic multiplicity}(\alpha)$ and 
		\[
			\Eig_\alpha(T)\cap \Eig_\beta(T)=\{\vec 0\}
		\] whenever $\alpha\neq \beta$.
	\end{theorem}

	\question
	Let $\mathcal V$ be a finite-dimensional vector space over an algebraically closed
	field and let $T:\mathcal V\to\mathcal V$
	be a linear transformation.
	\begin{parts}
		\item Show that $(T-\alpha id)(T-\beta id) = (T-\beta id)(T-\alpha id)$. Is it true
			that $(T-\alpha id)^a(T-\beta id)^b = (T-\beta id)^b(T-\alpha id)^a$?
		\item Show that $\Eig_{\lambda}(T)$ is an invariant subspace.
		\item Does $\mathcal V=\Eig_{\lambda_1}(T)\oplus\cdots\oplus \Eig_{\lambda_k}(T)$ where
			$\lambda_1,\ldots,\lambda_k$ are the eigenvalues? Why or why not?
		\item Can you come up with neccessary and sufficient conditions for a transformation
			$S:\mathcal V\to\mathcal V$ to be conjugate to $T$?
	\end{parts}

	\begin{definition}[Jordan Form]
		A square matrix $A$ is in \emph{Jordan form} if the diagonal of $A$ consists
		of Jordan blocks and the rest of the entries of $A$ are $0$.
	\end{definition}

	\question
	Let $T:\R^4\to\R^4$ and suppose $\Eig_2(T)$ and $\Eig_3(T)$ are both cyclic and two dimensional.
	\begin{parts}
		\item Does $[T]_{\mathscr E}^{\mathscr E}$ have a Jordan form? If so, write it down.
		\item Is the Jordan form for $[T]_{\mathscr E}^{\mathscr E}$ unique?
		\item Consider the matrix transformation $S:\R^4\to\R^4$ given by 
			$[S]_{\mathscr E}^{\mathscr E} = \mat{2&2&0&0\\
			    0&2&0&0\\
			    0&0&3&6\\
			    0&0&0&3}$. Are $T$ and $S$ similar?
		\item Find a basis $\mathscr B$ so that $[S]_{\mathscr B}^{\mathscr B}$ is in Jordan form.
	\end{parts}

	\begin{theorem}
		Let $\mathcal V$ be a finite-dimensional vector space over an algebraicially closed
		field and let $T,S:\mathcal V\to\mathcal V$ be linear transformations. $T$ and $S$ are conjugate
		if and only if they have the same Jordan form (up to permutation of the Jordan blocks).
	\end{theorem}



\end{document}
